{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a438afd9-843d-42e7-a4c9-b3516a282df4",
   "metadata": {},
   "source": [
    "# Get parameters of best performing random forest models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351b7867-f43b-4e8a-ae22-fd48a506e5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from dask_ml.model_selection import GridSearchCV as gscv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f373aaa-c24b-4c1d-8a15-45e5c278b36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories\n",
    "dir02 = '../paper_deficit/output/02_dbase/'\n",
    "dir03 = '../paper_deficit/output/03_rf/'\n",
    "dir03p = '../paper_deficit/output/03_rf/files_params/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d3c0ef-b3f9-4817-94fe-7370c339c4cf",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca021eef-7a37-422a-b179-ddadf90c9a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "from dask_jobqueue import SLURMCluster\n",
    "from dask.distributed import Client\n",
    "import dask\n",
    "\n",
    "# Initialize dask\n",
    "cluster = SLURMCluster(\n",
    "    queue='compute',                      # SLURM queue to use\n",
    "    cores=48,                             # Number of CPU cores per job\n",
    "    memory='256 GB',                      # Memory per job\n",
    "    account='bm0891',                     # Account allocation\n",
    "    interface=\"ib0\",                      # Network interface for communication\n",
    "    walltime='04:00:00',                  # Maximum runtime per job\n",
    "    local_directory='../dask/',           # Directory for local storage\n",
    "    job_extra_directives=[                # Additional SLURM directives for logging\n",
    "        '-o ../dask/LOG_worker_%j.o',     # Output log\n",
    "        '-e ../dask/LOG_worker_%j.e'      # Error log\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Scale dask cluster\n",
    "cluster.scale(jobs=15)\n",
    "\n",
    "# Configurate dashboard url\n",
    "dask.config.config.get('distributed').get('dashboard').update(\n",
    "    {'link': '{JUPYTERHUB_SERVICE_PREFIX}/proxy/{port}/status'}\n",
    ")\n",
    "\n",
    "# Create client\n",
    "client = Client(cluster)\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06ce2b8-0e17-4e2f-af6d-1a82b8179dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get database\n",
    "dbase = dd.read_parquet(dir02 + 'df_dbase.parquet')\n",
    "\n",
    "# Explanatory variables\n",
    "vars_exp = ['geom90m_convergence', 'geom90m_cti', 'geom90m_eastness',\n",
    "            'geom90m_northness', 'geom90m_slope', 'geom90m_spi',\n",
    "            'soilgrids2017_bdricm', 'soilgrids2017_bdrlog',\n",
    "            'soilgrids2017_bdticm', \n",
    "            'soilgrids2020_cec', 'soilgrids2020_cfvo', 'soilgrids2020_clay', \n",
    "            'soilgrids2020_phh2o', 'soilgrids2020_sand', 'soilgrids2020_silt',\n",
    "            'worldclim_bio1', 'worldclim_bio3', 'worldclim_bio4',\n",
    "            'worldclim_bio5', 'worldclim_bio6', 'worldclim_bio12', \n",
    "            'worldclim_bio13', 'worldclim_bio14', 'worldclim_bio15', \n",
    "            'worldclim_elev']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f95081-47c3-4f69-9931-3c852612feb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_params_rank(var_tar, scen):\n",
    "   \n",
    "    \"\"\"Find 10 best parameters for random forest regression in \n",
    "       pre-defined parameter space\n",
    "    \"\"\"\n",
    "\n",
    "    # Get training data\n",
    "    df_train = dbase[dbase['train_' + scen] == True][[var_tar, *vars_exp]] \\\n",
    "        .repartition(partition_size='200 MiB') \\\n",
    "        .persist()\n",
    "    \n",
    "    # Split training data in features and target/label\n",
    "    X_train = df_train[vars_exp]\n",
    "    y_train = df_train[var_tar]\n",
    "    \n",
    "    # Create pipline for randomforest regressor, needed for gridserachcv\n",
    "    pipe = make_pipeline(\n",
    "        # StandardScaler(),\n",
    "        RandomForestRegressor(),\n",
    "        )\n",
    "        \n",
    "    # Select parameters of grid\n",
    "    param_grid = dict(\n",
    "        #randomforestregressor__max_depth = [50],\n",
    "        randomforestregressor__n_estimators = [100, 200, 300], #[50, 100, 150],\n",
    "        randomforestregressor__min_samples_leaf = [2, 3, 4, 5],\n",
    "        randomforestregressor__max_features = [2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "        randomforestregressor__random_state = [42]\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Grid search with cross validation\n",
    "    grid_results = gscv(pipe, param_grid, \n",
    "                        cv=5, # five-fold cross validation\n",
    "                        scoring='r2', # r2 as evaluation criteria # sklearn: R2 gives the same ranking as squared error.\n",
    "                        refit=False)\n",
    "    \n",
    "    # Fit models\n",
    "    grid_results_fit = grid_results.fit(X_train, y_train)\n",
    "    \n",
    "    # Extract rank_test_score and parameters\n",
    "    df_params_rank = pd.DataFrame.from_dict(grid_results_fit.cv_results_) \\\n",
    "        .sort_values('rank_test_score') \\\n",
    "        .iloc[:,-5:] \\\n",
    "        .reset_index(drop=True)\n",
    "    \n",
    "    # Rename columns\n",
    "    df_params_rank.columns = [\n",
    "        i[29:] if i.startswith('param_randomforestregressor__') \n",
    "        else i for i in df_params_rank.columns\n",
    "        ]\n",
    "    \n",
    "    # Export\n",
    "    df_params_rank.to_csv(\n",
    "        dir03p + 'df_params_rank_' + var_tar + '_' + scen + '.csv', \n",
    "        index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dac502-af09-4480-9b18-8b024753255a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get parameters for agbc\n",
    "for var_tar in ['agbc_min', 'agbc_mean', 'agbc_max']:\n",
    "    for scen in ['prim', 'secd']:\n",
    "        %time random_forest_params_rank(var_tar, scen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed11d171-ad45-4e7f-813a-9872f764afa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get parameters for bgbc\n",
    "for var_tar in ['bgbc_min', 'bgbc_mean', 'bgbc_max']:\n",
    "    for scen in ['prim', 'secd']:\n",
    "        %time random_forest_params_rank(var_tar, scen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209c8e94-f526-448e-b580-e66695570715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get paramters for soc\n",
    "for var_tar in ['soc_min', 'soc_mean', 'soc_max']:\n",
    "    for scen in ['prim', 'secd']:\n",
    "        %time random_forest_params_rank(var_tar, scen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036dac7e-3377-45c8-957c-4fe362d2a90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad206dba-2c08-454a-b6ff-f86a15b4714f",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (deficit)",
   "language": "python",
   "name": "deficit_python312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

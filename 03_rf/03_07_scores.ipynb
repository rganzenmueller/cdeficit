{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "454c9c14-f86e-4721-9ec3-00ea317c17fa",
   "metadata": {},
   "source": [
    "# Calculate statistical metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1152c165-7533-42be-a097-803d51a8e699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics as metrics\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b634e085-da80-4459-aeb4-98bcad9be5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories\n",
    "dir02 = '../paper_deficit/output/02_dbase/'\n",
    "dir03 = '../paper_deficit/output/03_rf/'\n",
    "dir03p = os.path.join(dir03 + 'files_predicted/')\n",
    "dir03a = os.path.join(dir03 + 'files_adjusted/')\n",
    "dir03s = os.path.join(dir03 + 'files_scores/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fc24ee-c2a1-447a-9738-839a372c07e9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ff0478-dc83-459b-a000-802965583468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "from dask_jobqueue import SLURMCluster\n",
    "from dask.distributed import Client\n",
    "import dask\n",
    "\n",
    "# Initialize dask\n",
    "cluster = SLURMCluster(\n",
    "    queue='compute',                      # SLURM queue to use\n",
    "    cores=24,                             # Number of CPU cores per job\n",
    "    memory='256 GB',                      # Memory per job\n",
    "    account='bm0891',                     # Account allocation\n",
    "    interface=\"ib0\",                      # Network interface for communication\n",
    "    walltime='02:00:00',                  # Maximum runtime per job\n",
    "    local_directory='../dask/',           # Directory for local storage\n",
    "    job_extra_directives=[                # Additional SLURM directives for logging\n",
    "        '-o ../dask/LOG_worker_%j.o',     # Output log\n",
    "        '-e ../dask/LOG_worker_%j.e'      # Error log\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Scale dask cluster\n",
    "cluster.scale(jobs=1)\n",
    "\n",
    "# Configurate dashboard url\n",
    "dask.config.config.get('distributed').get('dashboard').update(\n",
    "    {'link': '{JUPYTERHUB_SERVICE_PREFIX}/proxy/{port}/status'}\n",
    ")\n",
    "\n",
    "# Create client\n",
    "client = Client(cluster)\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8e1031-5f2f-4180-82f6-334c684d397c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dbase file\n",
    "df_dbase = dd.read_parquet(os.path.join(dir02, 'df_dbase.parquet')) \\\n",
    "    .repartition(partition_size='1000 MiB') \\\n",
    "    .persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879aa150-8eb5-4425-b4f6-9e8bcb3c7943",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_scores(var_tar, scen):\n",
    "    \"\"\"\n",
    "    Calculates statistical metrics for Random Forest predictions.\n",
    "    \n",
    "    Parameters:\n",
    "        var_tar (str): Target variable name.\n",
    "        scen (str): Scenario name.\n",
    "    \n",
    "    Returns:\n",
    "        list: [R2, RMSE, MAE, MSE, Max Error] rounded to 2 decimal places.\n",
    "    \"\"\"\n",
    "    # Load Random Forest prediction data\n",
    "    input_file = os.path.join(dir03p, f\"df_rfpred_{var_tar}_{scen}.parquet\")\n",
    "    df_rfpred = dd.read_parquet(input_file)\n",
    "    \n",
    "    # Calculate the mean of Random Forest prediction columns\n",
    "    rfr_columns = [col for col in df_rfpred.columns if col.startswith('rfr_')]\n",
    "    df_rfpred['rfr_mean'] = df_rfpred[rfr_columns].mean(axis=1)\n",
    "    \n",
    "    # Select relevant columns from df_dbase\n",
    "    df_dbase_sel = df_dbase[['lat', 'lon', f'pot_{scen}', f'train_{scen}', var_tar]]\n",
    "    \n",
    "    # Merge base data with Random Forest predictions\n",
    "    df_merged = df_dbase_sel.merge(\n",
    "        df_rfpred[['lat', 'lon', 'rfr_mean']],\n",
    "        on=['lat', 'lon'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Filter for testing data (potential but not training)\n",
    "    test_data = df_merged[\n",
    "        (df_merged[f'pot_{scen}']) & (~df_merged[f'train_{scen}'])\n",
    "    ]\n",
    "    \n",
    "    # Extract original and predicted values\n",
    "    test_orig = test_data[var_tar].persist()\n",
    "    test_pred = test_data['rfr_mean'].persist()\n",
    "    \n",
    "    # Compute statistical metrics\n",
    "    metrics_dict = {\n",
    "        \"R2\": metrics.r2_score(test_orig, test_pred),\n",
    "        \"RMSE\": metrics.root_mean_squared_error(test_orig, test_pred),\n",
    "        \"MAE\": metrics.mean_absolute_error(test_orig, test_pred),\n",
    "        \"MSE\": metrics.mean_squared_error(test_orig, test_pred),\n",
    "        \"Max Error\": metrics.max_error(test_orig, test_pred)\n",
    "    }\n",
    "    \n",
    "    # Round metrics to 2 decimal places and return as a list\n",
    "    return [round(value, 2) for value in metrics_dict.values()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e585f4d9-379d-48f1-9573-66d0f0608dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Create dataframe with score values\n",
    "df_score = pd.DataFrame(\n",
    "    columns=['var_tar', 'scen', 'r2', 'rmse', 'mae', 'mse', 'max_error'])\n",
    "\n",
    "for var_tar in ['agbc_min', 'agbc_mean', 'agbc_max',\n",
    "                'bgbc_min', 'bgbc_mean', 'bgbc_max',\n",
    "                'soc_min', 'soc_mean', 'soc_max',]:\n",
    "    for scen in ['prim', 'secd']:\n",
    "        df_score.loc[len(df_score)] = [var_tar, scen, *get_scores(var_tar, scen)]\n",
    "\n",
    "# Export dataframe\n",
    "df_score.to_csv(os.path.join(dir03s, 'df_score_rfr_mean.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dc4f68-9f90-41da-9431-cd6ddaed42d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (deficit)",
   "language": "python",
   "name": "deficit_python312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

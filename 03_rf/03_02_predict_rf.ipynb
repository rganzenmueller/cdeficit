{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db44e5c5-b3dc-46e1-a6a1-dc9a2760bb65",
   "metadata": {},
   "source": [
    "# Predict carbon with random forest models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebb8882-359b-47cf-bb1d-b4d8c25b0fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import os, shutil, time\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from dask_ml.wrappers import ParallelPostFit\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbc4662-5220-4e86-a3ec-a84ea244d948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories\n",
    "dir02 = '../paper_deficit/output/02_dbase/'\n",
    "dir03 = '../paper_deficit/output/03_rf/'\n",
    "dir03p = '../paper_deficit/output/03_rf/files_predicted/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee31d9a7-7e1b-49ed-ae0e-e43d83170750",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae7db16-dabd-409c-a1fd-36eaea46af67",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65794b1-567a-4de5-ae20-a1f0759eb7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "from dask_jobqueue import SLURMCluster\n",
    "from dask.distributed import Client\n",
    "import dask\n",
    "\n",
    "# Initialize dask\n",
    "cluster = SLURMCluster(\n",
    "    queue='compute',                      # SLURM queue to use\n",
    "    cores=24,                             # Number of CPU cores per job\n",
    "    memory='256 GB',                      # Memory per job\n",
    "    account='bm0891',                     # Account allocation\n",
    "    interface=\"ib0\",                      # Network interface for communication\n",
    "    walltime='08:00:00',                  # Maximum runtime per job\n",
    "    local_directory='../dask/',           # Directory for local storage\n",
    "    job_extra_directives=[                # Additional SLURM directives for logging\n",
    "        '-o ../dask/LOG_worker_%j.o',     # Output log\n",
    "        '-e ../dask/LOG_worker_%j.e'      # Error log\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Scale dask cluster\n",
    "cluster.scale(jobs=2)\n",
    "\n",
    "# Configurate dashboard url\n",
    "dask.config.config.get('distributed').get('dashboard').update(\n",
    "    {'link': '{JUPYTERHUB_SERVICE_PREFIX}/proxy/{port}/status'}\n",
    ")\n",
    "\n",
    "# Create client\n",
    "client = Client(cluster)\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdea5e76-b8e9-4c7b-8c88-f957a7a17d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dbase file\n",
    "df_dbase = dd.read_parquet(os.path.join(dir02, \"df_dbase.parquet\")) \\\n",
    "    .repartition(partition_size='1000 MiB') \\\n",
    "    .persist()\n",
    "\n",
    "\n",
    "# Pandas dataframe with lat and lon values\n",
    "df_latlon = df_dbase[['lat', 'lon']].compute()\n",
    "\n",
    "# Explanatory variables\n",
    "vars_exp = ['geom90m_convergence', 'geom90m_cti', 'geom90m_eastness',\n",
    "            'geom90m_northness', 'geom90m_slope', 'geom90m_spi',\n",
    "            'soilgrids2017_bdricm', 'soilgrids2017_bdrlog',\n",
    "            'soilgrids2017_bdticm', \n",
    "            'soilgrids2020_cec', 'soilgrids2020_cfvo', 'soilgrids2020_clay', \n",
    "            'soilgrids2020_phh2o', 'soilgrids2020_sand', 'soilgrids2020_silt',\n",
    "            'worldclim_bio1', 'worldclim_bio3', 'worldclim_bio4',\n",
    "            'worldclim_bio5', 'worldclim_bio6', 'worldclim_bio12', \n",
    "            'worldclim_bio13', 'worldclim_bio14', 'worldclim_bio15', \n",
    "            'worldclim_elev']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf5f012-faf4-4e84-8bda-fdfc0340cb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_postfit_predict(X_train, y_train, X_predict, params):\n",
    "    \"\"\"\n",
    "    Fits a ParallelPostFit RandomForestRegressor model with the given parameters\n",
    "    and writes predictions to a Parquet file.\n",
    "\n",
    "    Parameters:\n",
    "        X_train: Training feature set\n",
    "        y_train: Training target set\n",
    "        X_predict: Features to make predictions on\n",
    "        params: Dictionary of model parameters\n",
    "        output_path: Path to save the output Parquet file\n",
    "    \"\"\"\n",
    "    # Initialize and fit the model\n",
    "    rfr = ParallelPostFit(\n",
    "        estimator=RandomForestRegressor(\n",
    "            min_samples_leaf=params[\"min_samples_leaf\"],\n",
    "            max_features=params[\"max_features\"],\n",
    "            n_estimators=params[\"n_estimators\"],\n",
    "            random_state=params[\"random_state\"],\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "        scoring='r2'\n",
    "    )\n",
    "    rfr.fit(X_train, y_train)\n",
    "\n",
    "    # Return predictions\n",
    "    return rfr.predict(X_predict)\n",
    "\n",
    "\n",
    "def random_forest_predict(var_tar, scen):\n",
    "\n",
    "    \"\"\"Predict carbon values for 10 best performing models and\n",
    "    export as parquet file\"\"\"\n",
    "    \n",
    "    # Get dataframe with parameters and ranks\n",
    "    df_params = pd.read_csv(\n",
    "         os.path.join(dir03, f\"files_params/df_params_rank_{var_tar}_{scen}.csv\"))\n",
    "    \n",
    "    # Get training data\n",
    "    df_train = df_dbase[df_dbase['train_' + scen] == True]# \\.repartition(partition_size='1000 MiB')\n",
    "    \n",
    "    # Define x_train and y_train \n",
    "    X_train = df_train[vars_exp].to_dask_array(lengths=True).compute()\n",
    "    y_train = df_train[var_tar].to_dask_array(lengths=True).compute()\n",
    "    \n",
    "    # Select columns of explanatory variables\n",
    "    X_predict = df_dbase[vars_exp].to_dask_array(lengths=True).persist()\n",
    "\n",
    "    df_latlonx = df_latlon\n",
    "    \n",
    "    # Predict and export values for best 10 models\n",
    "    for rank in range(1, 11):\n",
    "        # Filter parameters for the current rank\n",
    "        df_params_rank = df_params[df_params.rank_test_score == rank]\n",
    "        \n",
    "        # Extract parameters as a dictionary\n",
    "        params = {\"min_samples_leaf\": df_params_rank.min_samples_leaf.item(),\n",
    "                  \"max_features\": df_params_rank.max_features.item(),\n",
    "                  \"n_estimators\": df_params_rank.n_estimators.item(),\n",
    "                  \"random_state\": df_params_rank.random_state.item()\n",
    "                 }\n",
    "        \n",
    "        # Run the prediction\n",
    "        rfr_predict = parallel_postfit_predict(\n",
    "            X_train, y_train, X_predict, params)\n",
    "\n",
    "        # Add as array to dataframe with lat and lon\n",
    "        df_latlonx['rfr_' + str(rank)] = rfr_predict.compute()\n",
    "\n",
    "    # Construct output file name\n",
    "    output_file = os.path.join(dir03p, f\"df_rfpred_{var_tar}_{scen}.parquet\")\n",
    "    \n",
    "    # Remove output_file if already exists\n",
    "    if os.path.exists(output_file):\n",
    "        os.remove(output_file)\n",
    "\n",
    "    # Export as parquet file\n",
    "    df_latlonx.to_parquet(output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fd980e-db33-4bfe-92eb-076240b1549b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict values of agbc\n",
    "for var_tar in ['agbc_min', 'agbc_mean', 'agbc_max']:\n",
    "    for scen in ['prim', 'secd']:\n",
    "        %time random_forest_predict(var_tar, scen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a18821-a899-4f2e-b8f2-797cc9a7a5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict values of bgbc\n",
    "for var_tar in ['bgbc_min', 'bgbc_mean', 'bgbc_max']:\n",
    "    for scen in ['prim', 'secd']:\n",
    "        %time random_forest_predict(var_tar, scen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1c3978-2c4f-4801-8295-dfc5e57a6655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict values of soc\n",
    "for var_tar in ['soc_min', 'soc_mean', 'soc_max']:\n",
    "    for scen in ['prim', 'secd']:\n",
    "        %time random_forest_predict(var_tar, scen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7b9827-3501-4af9-acc4-48fea38515d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (deficit)",
   "language": "python",
   "name": "deficit_python312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2e97e70-612c-46d0-b1c9-93cc1ce0e7e6",
   "metadata": {},
   "source": [
    "# Predict carbon with quantile random forest models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad33cdef-54c5-4496-ba44-b5bde1569fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import os,  shutil, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from quantile_forest import RandomForestQuantileRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc022dc5-bf81-4b3d-9ddd-9ed6c6184d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories\n",
    "dir02 = '../paper_deficit/output/02_dbase/'\n",
    "dir03 = '../paper_deficit/output/03_rf/'\n",
    "dir03p = '../paper_deficit/output/03_rf/files_predicted/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69fd7d9-769a-4113-ac82-36912eb1090e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't show warnings\n",
    "# Dask gives \"UserWarning: Sending large graph ...\"\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c8e487-62bb-4cfd-8e84-5cc2fd243b58",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dffa926-d377-4e77-97ed-17ab08ab91d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "from dask_jobqueue import SLURMCluster\n",
    "from dask.distributed import Client\n",
    "import dask\n",
    "\n",
    "# Initialize dask\n",
    "cluster = SLURMCluster(\n",
    "    queue='compute',                      # SLURM queue to use\n",
    "    cores=24,                             # Number of CPU cores per job\n",
    "    memory='256 GB',                      # Memory per job\n",
    "    account='bm0891',                     # Account allocation\n",
    "    interface=\"ib0\",                      # Network interface for communication\n",
    "    walltime='08:00:00',                  # Maximum runtime per job\n",
    "    local_directory='../dask/',           # Directory for local storage\n",
    "    job_extra_directives=[                # Additional SLURM directives for logging\n",
    "        '-o ../dask/LOG_worker_%j.o',     # Output log\n",
    "        '-e ../dask/LOG_worker_%j.e'      # Error log\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Scale dask cluster\n",
    "cluster.scale(jobs=2)\n",
    "\n",
    "# Configurate dashboard url\n",
    "dask.config.config.get('distributed').get('dashboard').update(\n",
    "    {'link': '{JUPYTERHUB_SERVICE_PREFIX}/proxy/{port}/status'}\n",
    ")\n",
    "\n",
    "# Create client\n",
    "client = Client(cluster)\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c0fc81-ae21-4789-9316-b7af9adf397c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dbase file\n",
    "df_dbase = dd.read_parquet(dir02 + 'df_dbase.parquet') \\\n",
    "    .repartition(partition_size='1000 MiB') \\\n",
    "    .persist()\n",
    "\n",
    "# Explanatory variables\n",
    "vars_exp = ['geom90m_convergence', 'geom90m_cti', 'geom90m_eastness',\n",
    "            'geom90m_northness', 'geom90m_slope', 'geom90m_spi',\n",
    "            'soilgrids2017_bdricm', 'soilgrids2017_bdrlog',\n",
    "            'soilgrids2017_bdticm', \n",
    "            'soilgrids2020_cec', 'soilgrids2020_cfvo', 'soilgrids2020_clay', \n",
    "            'soilgrids2020_phh2o', 'soilgrids2020_sand', 'soilgrids2020_silt',\n",
    "            'worldclim_bio1', 'worldclim_bio3', 'worldclim_bio4',\n",
    "            'worldclim_bio5', 'worldclim_bio6', 'worldclim_bio12', \n",
    "            'worldclim_bio13', 'worldclim_bio14', 'worldclim_bio15', \n",
    "            'worldclim_elev']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2679332b-f474-44a1-8d0c-35ac0b045e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_qrfr(var_tar, scen):\n",
    "\n",
    "    \"\"\"Predict carbon densities using Quantile Random Forest Regression with \n",
    "        best performing parameter combination\"\"\"\n",
    "    \n",
    "    # Output file name\n",
    "    file_out = f'df_rfqpred_{var_tar}_{scen}.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101c0c41-3959-44d8-b56a-89ba07135162",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_random_forest_predict(var_tar, scen):\n",
    "\n",
    "    \"\"\"Predict carbon densities using Quantile Random Forest Regression with \n",
    "        best performing parameter combination\"\"\"\n",
    "    \n",
    "    # Output file name\n",
    "    file_out = f'df_rfqpred_{var_tar}_{scen}.parquet'\n",
    "    # Delete intermediate file directory from previous run if exist\n",
    "    if os.path.exists(os.path.join(dir03p, 'interm')):\n",
    "        shutil.rmtree(os.path.join(dir03p, 'interm'))    \n",
    "    # Create intermediate file directory\n",
    "    os.mkdir(os.path.join(dir03p, 'interm'))\n",
    "    # Delete qrfr file from old run if exists\n",
    "    if os.path.exists(os.path.join(dir03p, file_out)):\n",
    "        os.remove(os.path.join(dir03p, file_out))\n",
    "    \n",
    "    # Get training data\n",
    "    df_train = df_dbase[df_dbase['train_' + scen] == True]\n",
    "        \n",
    "    # Define x_train and y_train \n",
    "    X_train = df_train[vars_exp].to_dask_array(lengths=True).compute()\n",
    "    y_train = df_train[var_tar].to_dask_array(lengths=True).compute()\n",
    "    \n",
    "    npartitions = 5000 #5000\n",
    "    npgroups = 50 #50\n",
    "    # For testing uncomment next two lines \n",
    "    #npartitions = 200\n",
    "    #df_dbase = dd.from_pandas(df_dbase.head(2000000), npartitions = npartitions)\n",
    "    \n",
    "    # Extract variables, repartition, and export\n",
    "    df_dbase[vars_exp] \\\n",
    "        .repartition(npartitions=npartitions) \\\n",
    "        .to_parquet(os.path.join(dir03p,'interm', 'df_dbase_qrfr_interm.parquet')) \n",
    "    \n",
    "    # Read extratcted variables\n",
    "    X_predict = dd.read_parquet(\n",
    "        os.path.join(dir03p,'interm', 'df_dbase_qrfr_interm.parquet')) \\\n",
    "        .to_dask_array(lengths=True) \\\n",
    "        .persist()\n",
    "    \n",
    "    # Get parameters of model with best performance\n",
    "    file_params =  f'df_params_rank_{var_tar}_{scen}.csv'\n",
    "    df_params = pd.read_csv(os.path.join(dir03, 'files_params', file_params))\n",
    "    \n",
    "    df_params_sel = df_params[df_params.rank_test_score == 1]\n",
    "    \n",
    "    # Define random forest regressor\n",
    "    rfqr = RandomForestQuantileRegressor(\n",
    "        min_samples_leaf=df_params_sel.min_samples_leaf.item(),\n",
    "        max_features=df_params_sel.max_features.item(),\n",
    "        n_estimators=df_params_sel.n_estimators.item(),\n",
    "        random_state=df_params_sel.random_state.item(),\n",
    "        n_jobs=-1, \n",
    "        criterion='squared_error')\n",
    "    \n",
    "    # Fit random forest regressor\n",
    "    rfqr.fit(X_train, y_train, sparse_pickle=True)\n",
    "    \n",
    "    # List with each partition\n",
    "    list_partitions = [X_predict.partitions[i] \n",
    "                       for i in range(0, X_predict.npartitions)]\n",
    "    \n",
    "    # Group partitions in groups of npgroups\n",
    "    list_partition_groups = [list_partitions[i:i + npgroups] \n",
    "                             for i in range(0, len(list_partitions), npgroups)]\n",
    "    \n",
    "    # predict values of one partition\n",
    "    def predict_partition(p):\n",
    "        return rfqr.predict(p, quantiles=[0.05, 0.1, 0.5, 0.9, 0.95])\n",
    "    \n",
    "    # predict values of partition group (npgroups partitions) and return dataframe\n",
    "    def predict_pgroup(pgroup):\n",
    "        a = dask.compute([dask.delayed(predict_partition)(i.persist()) for i in pgroup])\n",
    "        b = a[0][0]\n",
    "    \n",
    "        for i in range(1, len(a[0])):\n",
    "            b = np.append(b, a[0][i],  axis=0)\n",
    "    \n",
    "        return pd.DataFrame(b, columns=['qrfr_005', 'qrfr_010', 'qrfr_050', 'qrfr_090', 'qrfr_095'])\n",
    "    \n",
    "    # predict partition groups\n",
    "    for i in range(0, len(list_partition_groups)):\n",
    "        file_interm = f'df_qrfr_xpredict_interm_{i}.parquet'\n",
    "        predict_pgroup(list_partition_groups[i]) \\\n",
    "            .to_parquet(os.path.join(dir03p, 'interm', file_interm))\n",
    "    \n",
    "    # Concat and export\n",
    "    def qrfr_interm_concat():\n",
    "    \n",
    "        # Import and concat interm files\n",
    "        a = pd.read_parquet(\n",
    "            os.path.join(dir03p, 'interm', f\"df_qrfr_xpredict_interm_0.parquet\"))\n",
    "        \n",
    "        for i in range(1, len(list_partition_groups)):\n",
    "            file_in = f'df_qrfr_xpredict_interm_{i}.parquet'\n",
    "            b = pd.read_parquet(\n",
    "                    os.path.join(dir03p, 'interm', file_in))\n",
    "            a = pd.concat([a, b], ignore_index=True)\n",
    "        \n",
    "        # Dataframe with lat and lon and qrfr values\n",
    "        c = df_dbase[['lat', 'lon']].compute().reset_index().drop('index', axis=1)\n",
    "        c['qrfr_005'] = a.qrfr_005\n",
    "        c['qrfr_010'] = a.qrfr_010\n",
    "        c['qrfr_050'] = a.qrfr_050\n",
    "        c['qrfr_090'] = a.qrfr_090\n",
    "        c['qrfr_095'] = a.qrfr_095\n",
    "        \n",
    "        # Export\n",
    "        c.to_parquet(os.path.join(dir03p, file_out))\n",
    "    \n",
    "    qrfr_interm_concat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d72151-9019-46cd-bde2-3f149c07ce67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict values of agbc\n",
    "for var_tar in ['agbc_min', 'agbc_mean', 'agbc_max']:\n",
    "    for scen in ['prim', 'secd']:\n",
    "        %time quantile_random_forest_predict(var_tar, scen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f3c3e7-f7b3-4d1e-8753-d3dce57064bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict values of bgbc\n",
    "for var_tar in ['bgbc_min', 'bgbc_mean', 'bgbc_max']:\n",
    "    for scen in ['prim', 'secd']:\n",
    "        %time quantile_random_forest_predict(var_tar, scen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d31e109-02fe-41cb-8515-97c89b60b403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict values of soc\n",
    "for var_tar in ['soc_min', 'soc_mean', 'soc_max']:\n",
    "    for scen in ['prim', 'secd']:\n",
    "        %time quantile_random_forest_predict(var_tar, scen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e89f12-bf03-4f9d-9611-ec58f0b6ae06",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (deficit)",
   "language": "python",
   "name": "deficit_python312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

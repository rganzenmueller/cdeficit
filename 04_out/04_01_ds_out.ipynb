{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44dc94f9-8fca-4f94-a01f-abe75b572d5c",
   "metadata": {},
   "source": [
    "# Create final output datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e66849c-f42a-4131-8d8f-91445a9b8511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import os, time, datetime\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f2fe7f-63c2-4281-877e-0ad72c060654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories\n",
    "dir01 = '../paper_deficit/output/01_prep/'\n",
    "dir02 = '../paper_deficit/output/02_dbase/'\n",
    "dir03 = '../paper_deficit/output/03_rf/'\n",
    "dir04 = '../paper_deficit/output/04_out/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b718ae8-600d-4ab8-848c-bae57d7de13b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831e7fdd-49c1-4904-8d2b-62a2d9659171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "from dask_jobqueue import SLURMCluster\n",
    "from dask.distributed import Client\n",
    "import dask\n",
    "\n",
    "# Initialize dask\n",
    "cluster = SLURMCluster(\n",
    "    queue='compute',                      # SLURM queue to use\n",
    "    cores=48,                             # Number of CPU cores per job\n",
    "    memory='256 GB',                      # Memory per job\n",
    "    account='bm0891',                     # Account allocation\n",
    "    interface=\"ib0\",                      # Network interface for communication\n",
    "    walltime='02:00:00',                  # Maximum runtime per job\n",
    "    local_directory='../dask/',           # Directory for local storage\n",
    "    job_extra_directives=[                # Additional SLURM directives for logging\n",
    "        '-o ../dask/LOG_worker_%j.o',     # Output log\n",
    "        '-e ../dask/LOG_worker_%j.e'      # Error log\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Scale dask cluster\n",
    "cluster.scale(jobs=5)\n",
    "\n",
    "# Configurate dashboard url\n",
    "dask.config.config.get('distributed').get('dashboard').update(\n",
    "    {'link': '{JUPYTERHUB_SERVICE_PREFIX}/proxy/{port}/status'}\n",
    ")\n",
    "\n",
    "# Create client\n",
    "client = Client(cluster)\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d316afc-1570-4dbb-a4f0-a2ab70458be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_rf(var_tar, scen, rf):\n",
    "    \"\"\"\n",
    "    Retrieve predicted carbon data from the rfaaa-file.\n",
    "\n",
    "    Args:\n",
    "        var_tar (str): The target variable (e.g., 'agbc_min').\n",
    "        scen (str): The scenario ('prim' or 'secd').\n",
    "\n",
    "    Returns:\n",
    "        xr.DataArray: DataArray containing modeled carbon data.\n",
    "    \"\"\"\n",
    "    if rf == 'rfaaa_mean':\n",
    "        file_in = os.path.join(\n",
    "            dir03 + f'/files_adjusted/ds_rfpred_{var_tar}_{scen}_rfaaa.zarr'\n",
    "            )\n",
    "    if rf in ['qrfr_005', 'qrfr_095']:\n",
    "        file_in = os.path.join(\n",
    "            dir03 + f'/files_predicted/ds_rfpred_{var_tar}_{scen}.zarr'\n",
    "            )\n",
    "    \n",
    "    da = xr.open_zarr(file_in)[rf]\n",
    "    da.attrs = dict(_FillValue=-32768)\n",
    "    return da\n",
    "\n",
    "\n",
    "def get_data_orig(var_tar, dir02):\n",
    "    \"\"\"\n",
    "    Retrieve the actual carbon data.\n",
    "\n",
    "    Args:\n",
    "        var_tar (str): The target variable (e.g., 'agbc_mean').\n",
    "        dir02 (str): Directory path where actual carbon data files are located.\n",
    "\n",
    "    Returns:\n",
    "        xr.DataArray: DataArray containing the original carbon data.\n",
    "    \"\"\"\n",
    "    ctype = var_tar.split('_')[0]\n",
    "    ds = xr.open_zarr(os.path.join(dir02, f'ds_prep_{ctype}.zarr'))\n",
    "    da = ds[var_tar].fillna(-32768).round(0).astype('int16')\n",
    "    da.attrs = dict(_FillValue=-32768)\n",
    "    return da\n",
    "\n",
    "\n",
    "def get_data_area(dir01):\n",
    "    \"\"\"\n",
    "    Retrieve the area data (in hectares) for each grid cell.\n",
    "\n",
    "    Args:\n",
    "        dir01 (str): Directory path where area data files are located.\n",
    "\n",
    "    Returns:\n",
    "        xr.DataArray: DataArray containing the area data.\n",
    "    \"\"\"\n",
    "    ds = xr.open_zarr(os.path.join(dir01, 'ds_prep_area_ha.zarr'))\n",
    "    da = ds.area_ha\n",
    "    da.attrs = dict(_FillValue='NaN')\n",
    "    return da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c545575-7139-4b60-bae0-0ce038ae306c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ds_ctype(ctype, complevel):\n",
    "    \"\"\"\n",
    "    Create output dataset for specific carbon type ('agbc', 'bgbc', 'soc').\n",
    "\n",
    "    Args:\n",
    "        ctype (str): The carbon type ('agbc', 'bgbc', or 'soc').\n",
    "        comlevel (int): Compression level (0-9): 1=best speed; 9=best compression\n",
    "    \"\"\"\n",
    "    ds = xr.Dataset()  # Create an empty xarray Dataset\n",
    "\n",
    "    # Add actual carbon data variables to the dataset\n",
    "    for var_tar in [f'{ctype}_min', f'{ctype}_mean', f'{ctype}_max']:\n",
    "        ds[f'{var_tar}_act'] = get_data_orig(var_tar, dir02)\n",
    "\n",
    "    # Add modeled carbon data variables for each scenario to the dataset\n",
    "    for scen in ['prim', 'secd']:\n",
    "        for var_tar in [f'{ctype}_min', f'{ctype}_mean', f'{ctype}_max']:\n",
    "            ds[f'{var_tar}_{scen}'] = get_data_rf(var_tar, scen, 'rfaaa_mean')\n",
    "            ds[f'{var_tar}_{scen}_q005'] = get_data_rf(var_tar, scen, 'qrfr_005')\n",
    "            ds[f'{var_tar}_{scen}_q095'] = get_data_rf(var_tar, scen, 'qrfr_095')\n",
    "\n",
    "    # Add area data to the dataset\n",
    "    ds['area_ha'] = get_data_area(dir01).astype('float32')\n",
    "\n",
    "    # Update latitude attributes\n",
    "    ds['lat'].attrs = dict(long_name=\"latitude\",\n",
    "                           units=\"degrees_north\",\n",
    "                           standard_name=\"latitude\",\n",
    "                           axis='X')\n",
    "\n",
    "    # Update longitude attributes\n",
    "    ds['lon'].attrs = dict(long_name=\"longitude\",\n",
    "                           units=\"degrees_east\",\n",
    "                           standard_name=\"longitude\",\n",
    "                           axis='Y')\n",
    "\n",
    "    # Update attributes of carbon variables based on type, metric, and scenario\n",
    "    for i in list(ds.data_vars):\n",
    "        if i.startswith(ctype):\n",
    "            if ctype == 'agbc':\n",
    "                str_ln_ctype = 'Above ground biomass carbon'\n",
    "            if ctype == 'bgbc':\n",
    "                str_ln_ctype = 'Below ground biomass carbon'\n",
    "            if ctype == 'soc':\n",
    "                str_ln_ctype = 'Soil organic carbon 0-30 cm'\n",
    "            if i.split('_')[1] == 'min':\n",
    "                str_ln_mid = 'Minimum scenario'\n",
    "            if i.split('_')[1] == 'mean':\n",
    "                str_ln_mid = 'Mean scenario'\n",
    "            if i.split('_')[1] == 'max':\n",
    "                str_ln_mid = 'Maximum scenario'\n",
    "            if i.split('_')[2] == 'act':\n",
    "                str_ln_scen = 'Actual'\n",
    "            if i.split('_')[2] == 'prim':\n",
    "                str_ln_scen = 'Pristine land assumption'\n",
    "            if i.split('_')[2] == 'secd':\n",
    "                str_ln_scen = 'Low human influence assumption'\n",
    "\n",
    "            # Combine components to create a descriptive long name\n",
    "            str_ln = f'{str_ln_ctype}; {str_ln_mid}; {str_ln_scen}'\n",
    "            if i.endswith('q005'):\n",
    "                str_ln = f'{str_ln_ctype}; {str_ln_mid}; {str_ln_scen}; 0.05 quantile'\n",
    "            if i.endswith('q095'):\n",
    "                str_ln = f'{str_ln_ctype}; {str_ln_mid}; {str_ln_scen}; 0.95 quantile'\n",
    "\n",
    "            # Update the variable's attributes\n",
    "            ds[i].attrs.update(dict(standard_name=i,\n",
    "                                    long_name=str_ln,\n",
    "                                    unit='Ct ha-1'))\n",
    "\n",
    "            # Update dataset title with the carbon type description\n",
    "            ds.attrs.update(dict(title=str_ln_ctype))\n",
    "\n",
    "    # Add attributes for the area variable\n",
    "    ds['area_ha'].attrs.update(dict(standard_name='area_ha',\n",
    "                                    long_name='Grid cell area',\n",
    "                                    unit='ha'))\n",
    "\n",
    "    # Add valid_min and valid_max attributes to each data variable\n",
    "    for i in ds[[i for i in ds.data_vars]]:\n",
    "        da_sel = ds[i].where(ds[i] != ds[i].attrs.get('_FillValue'))\n",
    "        ds[i].attrs.update(dict(valid_min=da_sel.min().compute().item(),\n",
    "                                valid_max=da_sel.max().compute().item()))\n",
    "\n",
    "    # Add general dataset attributes\n",
    "    ds.attrs.update(dict(\n",
    "        institution=f'Department of Geography, Ludwig-Maximilians-Universität München, Munich, Germany',\n",
    "        author='Raphael Ganzenmüller',\n",
    "        version='1.0',\n",
    "        date=f'{datetime.datetime.now():%Y-%m-%d %H:%M:%S%z}'  # Add current date and time\n",
    "    ))\n",
    "\n",
    "    # Load dataset to memory\n",
    "    ds = ds.persist()\n",
    "    \n",
    "    # Export the dataset as main and uncertainty datasets   \n",
    "    def export_data(ds_out, dir_out, file_out, complevel=4):\n",
    "        # Get encoding\n",
    "        encoding = {var: dict(zlib=True, complevel=complevel) for \n",
    "                    var in ds_out.data_vars}\n",
    "        # Export\n",
    "        ds_out.to_netcdf(os.path.join(dir_out, file_out),\n",
    "                         engine='netcdf4', encoding=encoding, mode='w') \n",
    "    \n",
    "    # Get quantile variables\n",
    "    l_q0xx = [i for i in ds.data_vars if i.endswith(('q005', 'q095'))]\n",
    "    \n",
    "    # Export main file\n",
    "    export_data(ds.drop_vars(l_q0xx), dir04, f'{ctype}.nc')\n",
    "    \n",
    "    # Export uncertainty file\n",
    "    export_data(ds[l_q0xx], dir04, f'{ctype}_unc.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12832bc9-0b4d-4edf-95a1-e721cfb41cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time create_ds_ctype('agbc', complevel=1)\n",
    "%time create_ds_ctype('bgbc', complevel=1)\n",
    "%time create_ds_ctype('soc', complevel=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3647a689-cb75-49eb-89f4-0116bf0b4ea7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8374ef98-13ea-4116-b633-382cb23a85ed",
   "metadata": {},
   "source": [
    "### Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65e8b9e-53fd-45a5-b3f5-1afe6e5d509c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f57a65-2f1c-4de8-92f4-ec4f45b593ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ds_out(file_out):\n",
    "    return xr.open_dataset(os.path.join(dir04, file_out), \n",
    "                          chunks='auto', decode_cf=False)\n",
    "\n",
    "def get_carbon_sum(ds):\n",
    "    return ((ds.where(ds != -32768) * ds.area_ha) \\\n",
    "                .sum(['lat', 'lon'])  * 1E-09).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5490b4-2092-414d-a171-1933a83695c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_agbc = get_ds_out('agbc.nc')\n",
    "ds_agbc_unc = get_ds_out('agbc_unc.nc')\n",
    "\n",
    "print(ds_agbc)\n",
    "print(get_carbon_sum(ds_agbc))\n",
    "print(ds_agbc_unc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5af692-7529-444e-b2e0-c5c0ad7cd313",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_bgbc = get_ds_out('bgbc.nc')\n",
    "ds_bgbc_unc = get_ds_out('bgbc_unc.nc')\n",
    "\n",
    "print(ds_bgbc)\n",
    "print(get_carbon_sum(ds_bgbc))\n",
    "print(ds_bgbc_unc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b51ad5-e125-4ca5-a7bb-30b9e94f444a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_soc = get_ds_out('soc.nc')\n",
    "ds_soc_unc = get_ds_out('soc_unc.nc')\n",
    "\n",
    "print(ds_soc)\n",
    "print(get_carbon_sum(ds_soc))\n",
    "print(ds_soc_unc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5acd063-3e68-4287-a70f-b90d7c4ca8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_main(ds):\n",
    "    ds = ds.where(ds != -32768).persist()\n",
    "    \n",
    "    fig, axs = plt.subplots(figsize=(20, 12), ncols=3, nrows=3)\n",
    "    axs = axs.ravel()\n",
    "    \n",
    "    for i in range(0, 9):\n",
    "        var_tar = [i for i in list(ds.data_vars) if i != 'area_ha'][i]\n",
    "        ds[var_tar].plot.imshow(ax=axs[i], robust=True)\n",
    "        axs[i].set_title(var_tar)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "def plot_unc(ds):\n",
    "    ds = ds.where(ds != -32768).persist()\n",
    "    \n",
    "    fig, axs = plt.subplots(figsize=(20, 15), ncols=3, nrows=4)\n",
    "    axs = axs.ravel()\n",
    "    \n",
    "    for i in range(0, 12):\n",
    "        v = list(ds.data_vars)[i]\n",
    "        ds[v].plot.imshow(ax=axs[i], robust=True)\n",
    "        axs[i].set_title(v)\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237d8e3e-4e37-4e3f-a3b4-9be5cd24b34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_main(ds_agbc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f49be2-f3ec-48f5-b6aa-030aae7936f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_unc(ds_agbc_unc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f54c374-01aa-45d8-a3ec-97f7bbe56715",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_main(ds_bgbc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3db0733-759f-4d2a-a3c1-e8f67d37b3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_unc(ds_bgbc_unc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77da83b-a018-4f41-ba78-bc2bf9f6fb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_main(ds_soc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad09a4f-2731-4a97-bddd-85c84dd441de",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_unc(ds_soc_unc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a981cfd-2e81-47b0-aeda-379ccfca3407",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (deficit)",
   "language": "python",
   "name": "deficit_python312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

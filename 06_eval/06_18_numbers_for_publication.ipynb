{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d8de6e9-c0e2-4aca-86ca-9cfe46c0b65b",
   "metadata": {},
   "source": [
    "# Calculate numbers for publication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98718c64-a288-4698-acf1-69e7f05b93c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7aa9ca-d90c-4089-a8a6-e97c3f7f9978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories\n",
    "dir01 = '../paper_deficit/output/01_prep/'\n",
    "dir04 = '../paper_deficit/output/04_out/'\n",
    "dir05 = '../paper_deficit/output/05_prep_other/'\n",
    "dir06 = '../paper_deficit/output/06_eval/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81ce48b-a38f-45bb-b84b-78c51407fea9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f870498c-057d-4ef2-bf0f-a1a06469db8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "from dask_jobqueue import SLURMCluster\n",
    "from dask.distributed import Client\n",
    "import dask\n",
    "\n",
    "# Initialize dask\n",
    "cluster = SLURMCluster(\n",
    "    queue='compute',                      # SLURM queue to use\n",
    "    cores=24,                             # Number of CPU cores per job\n",
    "    memory='256 GB',                      # Memory per job\n",
    "    account='bm0891',                     # Account allocation\n",
    "    interface=\"ib0\",                      # Network interface for communication\n",
    "    walltime='02:00:00',                  # Maximum runtime per job\n",
    "    local_directory='../dask/',           # Directory for local storage\n",
    "    job_extra_directives=[                # Additional SLURM directives for logging\n",
    "        '-o ../dask/LOG_worker_%j.o',     # Output log\n",
    "        '-e ../dask/LOG_worker_%j.e'      # Error log\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Scale dask cluster\n",
    "cluster.scale(jobs=2)\n",
    "\n",
    "# Configurate dashboard url\n",
    "dask.config.config.get('distributed').get('dashboard').update(\n",
    "    {'link': '{JUPYTERHUB_SERVICE_PREFIX}/proxy/{port}/status'}\n",
    ")\n",
    "\n",
    "# Create client\n",
    "client = Client(cluster)\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082c2842-074f-43d3-ade7-05f12f5f546d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6473e5c1-503e-490b-8eac-af6c5c2ac2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data predicted data\n",
    "ds_agbc = xr.open_dataset(os.path.join(dir04, 'agbc.nc')) \\\n",
    "    .chunk(dict(lat=5000, lon=5000)).persist()\n",
    "ds_bgbc = xr.open_dataset(os.path.join(dir04, 'bgbc.nc')) \\\n",
    "    .chunk(dict(lat=5000, lon=5000)).persist()\n",
    "ds_soc = xr.open_dataset(os.path.join(dir04, 'soc.nc')) \\\n",
    "    .chunk(dict(lat=5000, lon=5000)).persist()\n",
    "\n",
    "# Area array\n",
    "da_area = ds_agbc.area_ha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28e4fdc-b37d-4d60-afbe-7b4855d6dac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get hilda 2015 and lesiv forest management data\n",
    "ds_hilda = xr.open_mfdataset(os.path.join(dir01, 'ds_prep_hilda2015_*.zarr'),engine='zarr')\n",
    "ds_lesiv = xr.open_zarr(os.path.join(dir01, 'ds_prep_lesiv_nat.zarr'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41161390-a361-4de8-86c0-7a97caf63eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get land-sea mask, primary area, and area arrays at luh2 resolution\n",
    "ds_land_luh2res = xr.open_dataset(os.path.join(dir05, 'fig_dgvm', 'pot', 'ds_pot_land_luh2res.nc'))\n",
    "ds_prim_luh2res = xr.open_dataset(os.path.join(dir05, 'fig_dgvm', 'luh2', 'ds_luh2_prim_1700.nc'))\n",
    "ds_area_luh2res = xr.open_dataset(os.path.join(dir05, 'fig_dgvm', 'luh2', 'ds_luh2_grid_cell_area.nc'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf541958-34fe-457c-8b72-40778ee21387",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d29289-76fe-42eb-909b-ad8a453b5c54",
   "metadata": {},
   "source": [
    "### Get global carbon values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797cce12-27e9-4763-b63f-1b4c004a4028",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_carbon_values(ds):\n",
    "\n",
    "    \"\"\" Prepare dataframe with  global actual and potential min, mean, max \n",
    "    carbon values as PgC\"\"\"\n",
    "    \n",
    "    # Calculate carbon values and reshape to a DataFrame\n",
    "    df = (\n",
    "        (ds * ds.area_ha).sum() * 1E-09\n",
    "    ).drop_vars('area_ha').compute().round().astype('int16') \\\n",
    "        .to_pandas().to_frame('carbon_pgc').reset_index(names='v')\n",
    "\n",
    "    # Parse the 'v' column into ctype, stype, and atype, then pivot the table\n",
    "    df_pivot = (\n",
    "        df.assign(\n",
    "            ctype=[i.split('_')[0] for i in df.v],\n",
    "            stype=[i.split('_')[1] for i in df.v],\n",
    "            atype=[i.split('_')[2] for i in df.v]\n",
    "        )\n",
    "        .pivot(index=[\"ctype\", \"atype\"], columns=\"stype\", values=\"carbon_pgc\")\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Clean up column names\n",
    "    df_pivot.columns.name = None\n",
    "    df_pivot = df_pivot.rename_axis(None, axis=1)\n",
    "    df_pivot = df_pivot[['ctype', 'atype', 'min', 'mean', 'max']]\n",
    "\n",
    "    return df_pivot\n",
    "\n",
    "# Calculate carbon values for agbc, bgbc, and soc\n",
    "df_agbc_pgc =  get_carbon_values(ds_agbc)\n",
    "df_bgbc_pgc =  get_carbon_values(ds_bgbc)\n",
    "df_soc_pgc =  get_carbon_values(ds_soc)\n",
    "\n",
    "# Merge agbc, bgbc, soc values in one dataframe\n",
    "df_carbon_pgc = pd.concat([df_agbc_pgc, df_bgbc_pgc, df_soc_pgc]).reset_index(drop=True)\n",
    "df_carbon_pgc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68c6e63-b73c-44e8-af44-cf063f0868c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframes with global, min, mean, max, best and deficit estimates \n",
    "# for all carbon pools and aggregated carbon pools\n",
    "\n",
    "# Create empty dataframe\n",
    "df_carbon_pgc2 = pd.DataFrame(columns=['ctype', 'atype', 'metric', 'unit', 'value'])\n",
    "\n",
    "# Fill dataframe with loop over actual, primary and secondary dataframes\n",
    "for atype in ['act', 'prim', 'secd']:\n",
    "    # Select prepared dataframes\n",
    "    df_sel = df_carbon_pgc[df_carbon_pgc.atype == atype]\n",
    "    df_sel_agbc = df_sel[df_sel.ctype == 'agbc']\n",
    "    df_sel_bgbc = df_sel[df_sel.ctype == 'bgbc']\n",
    "    df_sel_soc = df_sel[df_sel.ctype == 'soc']\n",
    "\n",
    "    # Add min, mean, max estimates to df_carbon_pgc2\n",
    "    for ctype in ['agbc', 'bgbc', 'soc']:\n",
    "        for metric in ['min', 'mean', 'max']:\n",
    "            df_carbon_pgc2.loc[len(df_carbon_pgc2)] = [ctype, atype, metric, 'pgc', df_sel[df_sel.ctype == ctype][metric].item()]\n",
    "\n",
    "    # Add best guess to df_carbon_pgc2\n",
    "    df_carbon_pgc2.loc[len(df_carbon_pgc2)] = ['agbc', atype, 'best', 'pgc', df_sel_agbc['max'].item()]\n",
    "    df_carbon_pgc2.loc[len(df_carbon_pgc2)] = ['bgbc', atype, 'best', 'pgc', df_sel_bgbc['max'].item()]\n",
    "    df_carbon_pgc2.loc[len(df_carbon_pgc2)] = ['soc', atype, 'best', 'pgc', df_sel_soc['mean'].item()]\n",
    "\n",
    "    # Add cveg to df_carbon_pgc2\n",
    "    df_carbon_pgc2.loc[len(df_carbon_pgc2)] = ['cveg', atype, 'min', 'pgc', (df_sel_agbc['min'].item() + df_sel_bgbc['min'].item())]\n",
    "    df_carbon_pgc2.loc[len(df_carbon_pgc2)] = ['cveg', atype, 'mean', 'pgc', (df_sel_agbc['mean'].item() + df_sel_bgbc['mean'].item())]\n",
    "    df_carbon_pgc2.loc[len(df_carbon_pgc2)] = ['cveg', atype, 'max', 'pgc', (df_sel_agbc['max'].item() + df_sel_bgbc['max'].item())]\n",
    "    df_carbon_pgc2.loc[len(df_carbon_pgc2)] = ['cveg', atype, 'best', 'pgc', (df_sel_agbc['max'].item() + df_sel_bgbc['max'].item())]\n",
    "\n",
    "    # Add call (carbon all) to empty dataframe\n",
    "    df_carbon_pgc2.loc[len(df_carbon_pgc2)] = ['call', atype, 'min', 'pgc', (df_sel_agbc['min'].item() + df_sel_bgbc['min'].item() + df_sel_soc['min'].item())]\n",
    "    df_carbon_pgc2.loc[len(df_carbon_pgc2)] = ['call', atype, 'mean', 'pgc', (df_sel_agbc['mean'].item() + df_sel_bgbc['mean'].item() + df_sel_soc['mean'].item())]\n",
    "    df_carbon_pgc2.loc[len(df_carbon_pgc2)] = ['call', atype, 'max', 'pgc', (df_sel_agbc['max'].item() + df_sel_bgbc['max'].item() + df_sel_soc['max'].item())]\n",
    "    df_carbon_pgc2.loc[len(df_carbon_pgc2)] = ['call', atype, 'best', 'pgc', (df_sel_agbc['max'].item() + df_sel_bgbc['max'].item() + df_sel_soc['mean'].item())]\n",
    "\n",
    "# Make best, max, mean and min columns\n",
    "df_carbon_pgc2 = df_carbon_pgc2.pivot(index=[\"ctype\", \"atype\", \"unit\"], columns=\"metric\", values=\"value\").reset_index()\n",
    "\n",
    "# Create empty dataframe to store deficit percentage values\n",
    "df_deficit_pgc = pd.DataFrame(columns=df_carbon_pgc2.columns)\n",
    "\n",
    "# Create empty dataframe to store deficit percentage values\n",
    "df_deficit_per = pd.DataFrame(columns=df_carbon_pgc2.columns)\n",
    "\n",
    "# Calculate deficit in pgc and per and store in dataframes\n",
    "for ctype in ['agbc', 'bgbc', 'soc', 'cveg', 'call']:\n",
    "    df_carbon_pgc2_sel = df_carbon_pgc2[df_carbon_pgc2.ctype == ctype]\n",
    "    \n",
    "    row_act = df_carbon_pgc2_sel[df_carbon_pgc2_sel.atype == 'act'].iloc[0, 3:]\n",
    "    \n",
    "    for atype in ['prim', 'secd']:\n",
    "        row_ctype = df_carbon_pgc2_sel[df_carbon_pgc2_sel.atype == atype].iloc[0, 3:]\n",
    "        row_new_pgc = row_ctype - row_act\n",
    "        row_new_per = [round(i,2) for i in 1-(row_act / row_ctype)]\n",
    "        df_deficit_pgc.loc[len(df_deficit_pgc)] = [ctype, f'{atype}_deficit', 'pgc', *row_new_pgc]\n",
    "        df_deficit_per.loc[len(df_deficit_per)] = [ctype, f'{atype}_deficit', 'per', *row_new_per]\n",
    "\n",
    "\n",
    "# Calculate deficit of agbc, bgbc and soc as share of total deficit\n",
    "df_deficit_share_per = pd.DataFrame(columns=['ctype', 'atype', 'unit', 'best'])\n",
    "\n",
    "for atype in ['prim_deficit', 'secd_deficit']:\n",
    "    for ctype in ['agbc', 'bgbc', 'soc', 'cveg']:\n",
    "\n",
    "        v_call = df_deficit_pgc[((df_deficit_pgc.atype == atype) & \n",
    "                                 (df_deficit_pgc.ctype == 'call'))].best.item()\n",
    "\n",
    "        v_ctype = df_deficit_pgc[((df_deficit_pgc.atype == atype) & \n",
    "                                 (df_deficit_pgc.ctype == ctype))].best.item()\n",
    "\n",
    "        df_deficit_share_per.loc[len(df_deficit_share_per)] = [ctype, atype, 'per', round(v_ctype/v_call, 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5849f1b4-1e57-4df1-a45b-2ef27917c5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# carbon in pgc\n",
    "df_carbon_pgc2.to_csv(f'{dir06}csv/df_carbon_pgc.csv', index=False)\n",
    "df_carbon_pgc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b63f8f-ae6d-4514-800b-53d787e5cf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deficit in pgc\n",
    "df_deficit_pgc.to_csv(f'{dir06}csv/df_deficit_pgc.csv', index=False)\n",
    "df_deficit_pgc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fc199b-7eb2-44c1-986c-756ea41a6b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deficit in percent\n",
    "df_deficit_per.to_csv(f'{dir06}csv/df_deficit_per.csv', index=False)\n",
    "df_deficit_per"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64be4a7d-5b45-40ca-99ce-f23af59145eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Share of ctypes on deficit\n",
    "df_deficit_share_per.to_csv(f'{dir06}csv/df_deficit_share_per.csv', index=False)\n",
    "df_deficit_share_per"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fd52ee-da6d-41d8-8c64-2d39829b2239",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260b8bfe-552d-43d3-b811-60f656ca650f",
   "metadata": {},
   "source": [
    "### Get LUC carbon values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dadd23-6542-40d7-abc1-df525ec9387e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate actual, potential and deficit for vegetation and soil and combined\n",
    "def get_data(scen):\n",
    "    ds = xr.Dataset()\n",
    "    ds = ds.assign(\n",
    "        agbc_act = ds_agbc.agbc_max_act,\n",
    "        agbc_pot = ds_agbc[f'agbc_max_{scen}'],\n",
    "        bgbc_act = ds_bgbc.bgbc_max_act,\n",
    "        bgbc_pot = ds_bgbc[f'bgbc_max_{scen}'],\n",
    "        cveg_act = ds_agbc.agbc_max_act + ds_bgbc.bgbc_max_act,\n",
    "        cveg_pot = ds_agbc[f'agbc_max_{scen}'] + ds_bgbc[f'bgbc_max_{scen}'],\n",
    "        soc_act = ds_soc.soc_mean_act,\n",
    "        soc_pot = ds_soc[f'soc_mean_{scen}'])\n",
    "    \n",
    "    \n",
    "    ds = ds.assign(\n",
    "        cveg_def = ds.cveg_pot - ds.cveg_act,\n",
    "        soc_def = ds.soc_pot - ds.soc_act,\n",
    "        call_act = ds.cveg_act + ds.soc_act,\n",
    "        call_pot = ds.cveg_pot + ds.soc_pot,\n",
    "        call_def = ((ds.cveg_pot + ds.soc_pot) - (ds.cveg_act + ds.soc_act))\n",
    "    )\n",
    "    return ds\n",
    "\n",
    "\n",
    "ds_prim = get_data('prim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856a95b7-d503-4fc6-b5ac-7a7e2c3f1bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define that all forest that is not natural according to lesiv is human influenced\n",
    "da_forestn = ds_hilda.hilda2015_forest * ds_lesiv.lesiv_nat\n",
    "da_foresth = ds_hilda.hilda2015_forest - da_forestn\n",
    "ds_hilda = ds_hilda \\\n",
    "    .assign(hilda2015_forestn = da_forestn,\n",
    "            hilda2015_foresth = da_foresth) \\\n",
    "    .drop_vars('hilda2015_forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1144557-a40b-431e-bfcc-6bdbd0caa4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create hilda dataset with variables renamed\n",
    "ds_hilda2 = ds_hilda.rename({var: var.replace(\"hilda2015_\", \"\") for var in ds_hilda.data_vars})\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "df_luc_list = []\n",
    "\n",
    "# Loop over each variable and compute the corresponding DataFrame\n",
    "for var in ds_prim.data_vars:\n",
    "    temp_df = (((ds_hilda2 * ds_prim[var]) * da_area)\n",
    "               .sum(['lat', 'lon']) * 1e-9) \\\n",
    "               .compute() \\\n",
    "               .to_pandas() \\\n",
    "               .to_frame(var) \\\n",
    "               .transpose() \\\n",
    "               .reset_index(names='data_vars')\n",
    "    \n",
    "    df_luc_list.append(temp_df)\n",
    "\n",
    "# Concatenate all processed DataFrames\n",
    "df_luc = pd.concat(df_luc_list, ignore_index=True)\n",
    "\n",
    "# Add column luc_all_pgc with sum of all carbon attributed to luc areas\n",
    "df_luc = df_luc.assign(luc_pgc=df_luc[list(ds_hilda2.data_vars)].sum(axis=1).values)\n",
    "\n",
    "# Rename columns\n",
    "df_luc = df_luc.rename(columns={i: i + '_pgc' for i in list(ds_hilda2.data_vars)})\n",
    "\n",
    "# Export\n",
    "df_luc.round().to_csv(f'{dir06}csv/df_luc_pgc.csv', index=False)\n",
    "df_luc.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8327c1f1-6b3b-44f5-8dce-73350908eb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new dataframe with percentage values  \n",
    "df_per = df_luc.copy()  \n",
    "for col in [f'{i}_pgc' for i in list(ds_hilda2.data_vars)]:  \n",
    "    df_per[f'{col.split('_')[0]}_per'] = df_luc[col] / df_luc['luc_pgc']  \n",
    "\n",
    "# Keeping only required columns  \n",
    "df_per = df_per[[\"data_vars\"] + [i for i in df_per.columns if i.endswith('per')]]  \n",
    "\n",
    "# Export\n",
    "df_per.round(2).to_csv(f'{dir06}csv/df_deficit_luc_per.csv', index=False)\n",
    "df_per.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7e1449-69f8-4040-9772-b8ad69311e1f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926de7ef-37f7-413d-8180-fae35f9b9aa7",
   "metadata": {},
   "source": [
    "### Share of luc on deficit as percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644bf3ff-9749-4a66-81dd-96dae8e7a64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data from ecozone preparation\n",
    "df = pd.read_csv(os.path.join(dir05, 'fig_ecozone', 'df_ecozone_data_prim.csv'))\n",
    "\n",
    "# split cat column to new columns\n",
    "df = df.assign(eco = [i.split('_')[0] for i in df.cat], \n",
    "               luc = [i.split('_')[1] for i in df.cat],\n",
    "               ctype = [i.split('_')[2] for i in df.cat],\n",
    "               atype = [i.split('_')[3] for i in df.cat],)\n",
    "\n",
    "# Filter deficit estimates\n",
    "df_def = df[df.atype == 'def'][['carbon_pg', 'eco', 'luc', 'ctype']]\n",
    "\n",
    "# Pivot the table\n",
    "df_def = df_def.pivot(index=['eco', 'ctype'], columns='luc', values='carbon_pg') \\\n",
    "    .reset_index()\n",
    "\n",
    "# Renaming columns for clarity\n",
    "df_def.columns.name = None\n",
    "df_def = df_def.rename_axis(None, axis=1)\n",
    "\n",
    "# Reset index\n",
    "df_def = df_def.set_index(['eco', 'ctype'])\n",
    "\n",
    "# Add column with total deficit estimates from luc values\n",
    "df_def = df_def.assign(luc_all = df_def.sum(axis=1))\n",
    "\n",
    "# Create table with deficit as percentage\n",
    "df_def_per = df_def.div(df_def['luc_all'], axis=0).round(2).reset_index()\n",
    "\n",
    "# Filter total deficit rows\n",
    "df_def_per = df_def_per[df_def_per.ctype == 'call']\n",
    "\n",
    "# Export\n",
    "df_def_per.to_csv(f'{dir06}csv/df_deficit_luc_eco_per.csv', index=False)\n",
    "df_def_per"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccf58cc-20f8-42ce-9200-e4b42136876c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81112298-e993-4810-9e17-f0c2df672c30",
   "metadata": {},
   "source": [
    "### Average actual, potential and deficit (tha, percentage) per area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927aab28-e478-4f7f-bca1-06625cdcf436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stats luc classes (tha)\n",
    "df = pd.read_csv(os.path.join(dir05, 'fig_ecozone', 'df_ecozone_scatter_data_prim.csv'))\n",
    "df = df[df.cat0 == 'all'] \\\n",
    "    [['cat1', 'cat2', 'act_tha', 'pot_tha', 'def_tha']] \\\n",
    "    .assign(def_p = round(1-(df.act_tha / df.pot_tha), 2)) \\\n",
    "    .sort_values('cat1', ascending=True) \\\n",
    "    .sort_values('cat2', ascending=False)\n",
    "\n",
    "for i in ['act_tha', 'pot_tha', 'def_tha']:\n",
    "    df[i] = round(df[i]).astype('int')\n",
    "\n",
    "df.to_csv(f'{dir06}csv/df_deficit_luc_perarea.csv', index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b91a53a-eb54-463c-8896-e55c4cbf8b4d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd62ae6-2670-4aae-b410-8bef8e3c3273",
   "metadata": {},
   "source": [
    "### Get DGVM data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b2d8dc-b668-49c5-8440-8751f83e96f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get global dgvm and other data data for primary area\n",
    "df_dgvm = pd.read_csv(os.path.join(dir05, 'fig_dgvm', 'data_dgvm_global.csv'))\n",
    "\n",
    "df_dgvm = df_dgvm.assign(call_d = df_dgvm.cveg_d + df_dgvm.csoil_d)\n",
    "v_call_d_gan = df_dgvm[df_dgvm.model == 'ganzenmueller'].call_d.item()\n",
    "df_dgvm = df_dgvm.assign(call_d_diff2gan_per = 1-(df_dgvm.call_d / v_call_d_gan))\n",
    "\n",
    "# Rename columns\n",
    "new_columns = []\n",
    "\n",
    "for col in df_dgvm.columns:\n",
    "    if col == 'model':\n",
    "        new_columns.append(col)\n",
    "    elif col.endswith('_dp'):\n",
    "        new_columns.append(col.replace('_dp', '_per'))\n",
    "    elif not col.endswith('_dp') and not col.endswith('_per'):\n",
    "        new_columns.append(col + '_pgc')\n",
    "    else:\n",
    "        new_columns.append(col)\n",
    "\n",
    "df_dgvm.columns = new_columns\n",
    "\n",
    "df_dgvm.to_csv(f'{dir06}csv/df_deficit_dgvm.csv', index=False)\n",
    "df_dgvm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427e9b6c-05a4-4bd7-affe-6c1fc1175b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print relevant numbers\n",
    "list_dgvm = ['cablepop', 'classic', 'clm', 'dlem', 'ibis', 'isam', 'jsbach', \n",
    "             'jules', 'lpjguess', 'lpjwsl', 'orchidee']\n",
    "\n",
    "df_dgvm_model = df_dgvm[df_dgvm.model.isin(list_dgvm)]\n",
    "\n",
    "print(f'DGVM mean difference to our estimates (%): {round(np.mean(df_dgvm_model.call_d_diff2gan_per), 2)}')\n",
    "print(f'DGVM minimum difference to our estimates (%): {round(np.min(df_dgvm_model.call_d_diff2gan_per), 2)}')\n",
    "print(f'DGVM maximum difference to our estimates (%): {round(np.max(df_dgvm_model.call_d_diff2gan_per), 2)}')\n",
    "\n",
    "print(f'DGVM mean (PgC): {round(df_dgvm[df_dgvm.model == 'dgvm_mean'].call_d_pgc.item(), 2)}')\n",
    "print(f'DGVM SD (PgC): {round(np.std(df_dgvm_model.call_d_pgc), 2)}')\n",
    "\n",
    "print(f'DGVM cVeg mean (PgC): {round(df_dgvm[df_dgvm.model == 'dgvm_mean'].cveg_d_pgc.item(), 2)}')\n",
    "print(f'DGVM cVeg SD (PgC): {round(np.std(df_dgvm_model.cveg_d_pgc), 2)}')\n",
    "\n",
    "print(f'DGVM SOC mean (PgC): {round(df_dgvm[df_dgvm.model == 'dgvm_mean'].csoil_d_pgc.item(), 2)}')\n",
    "print(f'DGVM SOC SD (PgC): {round(np.std(df_dgvm_model.csoil_d_pgc), 2)}')\n",
    "\n",
    "# Evaluates to:\n",
    "# DGVM mean difference to our estimates (%): 0.37\n",
    "# DGVM minimum difference to our estimates (%): 0.02\n",
    "# DGVM maximum difference to our estimates (%): 0.58\n",
    "# DGVM mean (PgC): -171.15\n",
    "# DGVM SD (PgC): 52.25\n",
    "# DGVM cVeg mean (PgC): -134.07\n",
    "# DGVM cVeg SD (PgC): 45.02\n",
    "# DGVM SOC mean (PgC): -37.08\n",
    "# DGVM SOC SD (PgC): 31.88"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e70200-d6e2-4e00-aa31-2e29cce6dc1e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0411f528-8e43-4af1-bb3b-0060eb44df11",
   "metadata": {},
   "source": [
    "### LUH2 Primary land area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad93ea4-d195-4489-be73-21b34e4c398f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 1700 primary land area as ratio of total land area\n",
    "da_land_luh2res = ds_land_luh2res.land_sea_mask\n",
    "da_prim_luh2res = ds_prim_luh2res.prim_1700\n",
    "da_area_luh2res = ds_area_luh2res.grid_cell_area_ha\n",
    "\n",
    "area_prim = ((da_prim_luh2res * da_area_luh2res).sum(['lat', 'lon']) * 0.01).compute().item()\n",
    "area_land = ((da_land_luh2res * da_area_luh2res).sum(['lat', 'lon']) * 0.01).compute().item()\n",
    "\n",
    "# Print area\n",
    "print(f'LUH2 primary 1700 area vs. total land area : {round(area_prim / area_land, 3)}')\n",
    "\n",
    "# Evaluates to: \n",
    "# LUH2 primary 1700 area vs. total land area : 0.824"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7e619e-0578-458a-8e67-357f0ee85984",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0aa3d55-5009-4ebf-9262-21b413435d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (deficit)",
   "language": "python",
   "name": "deficit_python312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4bd6f01-d4f6-4528-b127-0e4cc241ece4",
   "metadata": {},
   "source": [
    "# Prepare data for Figure \"Ecozone\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62baaba-ba99-4f55-8438-1fc93aeb102a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbcd81d-56c2-4089-9e52-246f5028bc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories\n",
    "dir01 = '../paper_deficit/output/01_prep/'\n",
    "dir04 = '../paper_deficit/output/04_out/'\n",
    "dir05 = '../paper_deficit/output/05_prep_other/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7787d68d-db1a-4f95-993e-d47202c7c4bf",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a1006f-6cd6-40d4-8264-e34fcb5b8280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "from dask_jobqueue import SLURMCluster\n",
    "from dask.distributed import Client\n",
    "import dask\n",
    "\n",
    "# Initialize dask\n",
    "cluster = SLURMCluster(\n",
    "    queue='compute',                      # SLURM queue to use\n",
    "    cores=24,                             # Number of CPU cores per job\n",
    "    memory='256 GB',                      # Memory per job\n",
    "    account='bm0891',                     # Account allocation\n",
    "    interface=\"ib0\",                      # Network interface for communication\n",
    "    walltime='02:00:00',                  # Maximum runtime per job\n",
    "    local_directory='../dask/',           # Directory for local storage\n",
    "    job_extra_directives=[                # Additional SLURM directives for logging\n",
    "        '-o ../dask/LOG_worker_%j.o',     # Output log\n",
    "        '-e ../dask/LOG_worker_%j.e'      # Error log\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Scale dask cluster\n",
    "cluster.scale(jobs=5)\n",
    "\n",
    "# Configurate dashboard url\n",
    "dask.config.config.get('distributed').get('dashboard').update(\n",
    "    {'link': '{JUPYTERHUB_SERVICE_PREFIX}/proxy/{port}/status'}\n",
    ")\n",
    "\n",
    "# Create client\n",
    "client = Client(cluster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b5845b-a6d7-4037-9d3b-70a1cefd9afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data\n",
    "ds_area = xr.open_zarr(os.path.join(dir01, 'ds_prep_area_ha.zarr'))\n",
    "da_area = ds_area.area_ha\n",
    "\n",
    "ds_fao = xr.open_zarr(os.path.join(dir01, 'ds_prep_fao2010.zarr'))\n",
    "\n",
    "ds_hilda = xr.open_mfdataset(os.path.join(dir01, 'ds_prep_hilda2015_*.zarr'),\n",
    "                            engine='zarr')\n",
    "\n",
    "ds_lesiv = xr.open_zarr(os.path.join(dir01, 'ds_prep_lesiv_nat.zarr'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4a99c6-5cd1-4109-bb2f-39e1315fdc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get carbon densities from analysis\n",
    "ds_agbc = xr.open_dataset(os.path.join(dir04, 'agbc.nc')) \\\n",
    "    .chunk(dict(lat=5000, lon=5000)).persist()\n",
    "ds_bgbc = xr.open_dataset(os.path.join(dir04, 'bgbc.nc')) \\\n",
    "    .chunk(dict(lat=5000, lon=5000)).persist()\n",
    "ds_soc = xr.open_dataset(os.path.join(dir04, 'soc.nc')) \\\n",
    "    .chunk(dict(lat=5000, lon=5000)).persist()\n",
    "\n",
    "\n",
    "# Calculate actual, potential and deficit for vegetation and soil and combined\n",
    "def get_data(scen):\n",
    "    ds = xr.Dataset()\n",
    "    ds = ds.assign(\n",
    "        cveg_act = ds_agbc.agbc_max_act + ds_bgbc.bgbc_max_act,\n",
    "        cveg_pot = ds_agbc['agbc_max_' + scen] + ds_bgbc['bgbc_max_' + scen],\n",
    "        soc_act = ds_soc.soc_mean_act,\n",
    "        soc_pot = ds_soc['soc_mean_' + scen])\n",
    "    \n",
    "    \n",
    "    ds = ds.assign(\n",
    "        cveg_def = ds.cveg_pot - ds.cveg_act,\n",
    "        soc_def = ds.soc_pot - ds.soc_act,\n",
    "        call_act = ds.cveg_act + ds.soc_act,\n",
    "        call_pot = ds.cveg_pot + ds.soc_pot,\n",
    "        call_def = ((ds.cveg_pot + ds.soc_pot) - (ds.cveg_act + ds.soc_act))\n",
    "    )\n",
    "    return ds\n",
    "\n",
    "\n",
    "ds_prim = get_data('prim')\n",
    "ds_secd = get_data('secd') # secd def has negative grid cells <- actual <= primary but cases with actual > secondary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe41215-1e5a-487e-b46a-e3b2301c59dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carbon stocks our study\n",
    "ds_prim_sum = ((ds_prim * da_area).sum(['lat', 'lon']) * 1e-9).compute()\n",
    "ds_secd_sum = ((ds_secd * da_area).sum(['lat', 'lon']) * 1e-9).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8a2526-6fea-4eb2-8c9d-d7239a06228a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define that all forest that is not natural according to lesiv is human influenced\n",
    "da_forestn = ds_hilda.hilda2015_forest * ds_lesiv.lesiv_nat\n",
    "da_foresth = ds_hilda.hilda2015_forest - da_forestn\n",
    "ds_hilda = ds_hilda \\\n",
    "    .assign(hilda2015_forestn = da_forestn,\n",
    "            hilda2015_foresth = da_foresth) \\\n",
    "    .drop_vars('hilda2015_forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd65a48-a233-47a5-aa01-86ec74e409f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare fao2010\n",
    "ds_fao_split = xr.Dataset(\n",
    "    dict(fao2010_trop1 = xr.where(ds_fao.fao2010 == 1, 1, 0),\n",
    "         fao2010_trop2 = xr.where(ds_fao.fao2010 == 2, 1, 0),\n",
    "         fao2010_subt = xr.where(ds_fao.fao2010 == 3, 1, 0),\n",
    "         fao2010_temp = xr.where(ds_fao.fao2010 == 4, 1, 0),\n",
    "         fao2010_bore = xr.where(ds_fao.fao2010 == 5, 1, 0),\n",
    "         fao2010_pola = xr.where(ds_fao.fao2010 == 6, 1, 0))) \\\n",
    "    .drop_vars('spatial_ref') \\\n",
    "    .astype('bool') \\\n",
    "    .reindex_like(ds_prim, method='nearest') # ds_fao_split.lat.equals(ds_prim.lat) returns False. Not sure why"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92287927-fc18-4995-aec0-76ab5a3babe4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c7e87e-d5e9-41fa-8eb0-8c143e7bcc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds_prim\n",
    "\n",
    "# Dataset with carbon stocks for each combination of ecozones and land cover\n",
    "ds_split = xr.Dataset()\n",
    "for i in list(ds_fao_split.data_vars):\n",
    "        for iii in list(ds.data_vars):\n",
    "            ds_split[i[8:] + '_' + iii] = (\n",
    "                ds_fao_split[i] * ds[iii])\n",
    "\n",
    "# Dataframe with carbons stocks of each combination\n",
    "df_split = (ds_split * da_area * 1e-9) \\\n",
    "    .sum(['lat', 'lon']) \\\n",
    "    .compute() \\\n",
    "    .to_pandas() \\\n",
    "    .reset_index() \\\n",
    "    .rename(columns={'index':'cat', 0: 'carbon_pg'})\n",
    "\n",
    "# Prepare categories\n",
    "df_split['cat0'] = [i[0] for i in [i.split('_') for i in df_split.cat]]\n",
    "df_split['cat1'] = [i[1] for i in [i.split('_') for i in df_split.cat]]\n",
    "df_split['cat2'] = [i[2] for i in [i.split('_') for i in df_split.cat]]\n",
    "\n",
    "\n",
    "df_split = df_split \\\n",
    "    .drop('cat', axis=1) \\\n",
    "    .assign(cat12 = df_split.cat1 + '_' + df_split.cat2) \\\n",
    "    .drop(columns=['cat1', 'cat2']) \\\n",
    "    .pivot(columns=['cat12'], index=['cat0']) \\\n",
    "    .groupby(['cat0']) \\\n",
    "    .sum() \\\n",
    "    .droplevel(0, axis=1) \\\n",
    "    .reset_index() \\\n",
    "    [['cat0', 'cveg_act', 'cveg_pot', 'cveg_def', \n",
    "      'soc_act', 'soc_pot', 'soc_def', \n",
    "      'call_act', 'call_pot', 'call_def']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74e5c19-7563-473a-b0cf-4dd625ece2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets with carbon stocks for each combination of ecozones and land cover\n",
    "def prep_ecozone_data(scen):\n",
    "\n",
    "    # Select dataset\n",
    "    if scen == 'prim':\n",
    "        ds = ds_prim\n",
    "    if scen == 'secd':\n",
    "        ds = ds_secd\n",
    "        \n",
    "    # Dataset with carbon stocks for each combination of ecozones and land cover\n",
    "    ds_split = xr.Dataset()\n",
    "    for i in list(ds_fao_split.data_vars):\n",
    "        for ii in [i for i in list(ds_hilda.data_vars)]:\n",
    "            for iii in list(ds.data_vars):\n",
    "                ds_split[i[8:] + '_' + ii[10:] + '_' + iii] = (\n",
    "                    ds_fao_split[i] * ds_hilda[ii] * ds[iii])\n",
    "\n",
    "    # Dataframe with carbons stocks of each combination\n",
    "    df_split = (ds_split * da_area * 1e-9) \\\n",
    "        .sum(['lat', 'lon']) \\\n",
    "        .compute() \\\n",
    "        .to_pandas() \\\n",
    "        .reset_index() \\\n",
    "        .rename(columns={'index':'cat', 0: 'carbon_pg'})\n",
    "    \n",
    "    # Export\n",
    "    file_out = f'df_ecozone_data_{scen}.csv'\n",
    "    df_split.to_csv(os.path.join(dir05, 'fig4_ecozone', file_out), index=False)\n",
    "    \n",
    "\n",
    "%time prep_ecozone_data('prim')\n",
    "%time prep_ecozone_data('secd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32a168c-214a-4c79-8548-cdb8f3c4da78",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def prep_ecozone_area_data():\n",
    "\n",
    "    # Dataset with area for each combination of ecozones and land cover\n",
    "    ds_split_area = xr.Dataset()\n",
    "    for i in list(ds_fao_split.data_vars):\n",
    "        for ii in [i for i in list(ds_hilda.data_vars)]:\n",
    "            ds_split_area[i[8:] + '_' + ii[10:]] = ds_fao_split[i] * ds_hilda[ii]\n",
    "            \n",
    "    # dataframe with areas of each combination\n",
    "    df_split_area = (ds_split_area  * da_area) \\\n",
    "        .sum(['lat', 'lon']) \\\n",
    "        .compute() \\\n",
    "        .to_pandas() \\\n",
    "        .reset_index() \\\n",
    "        .rename(columns={'index':'cat', 0: 'area_ha'})\n",
    "    \n",
    "    # export\n",
    "    df_split_area.to_csv(\n",
    "        os.path.join(dir05, 'fig_ecozone/df_ecozone_area_data.csv'), \n",
    "        index=False)\n",
    "\n",
    "\n",
    "prep_ecozone_area_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0628c884-9544-4ff1-a183-1a94fc196466",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def prep_ecozone_scatter():\n",
    "    # Prepare scatter plot data\n",
    "    # Get carbon data\n",
    "    df_split = pd.read_csv(os.path.join(dir05, 'fig_ecozone/df_ecozone_data_prim.csv'))\n",
    "        \n",
    "    df_split['cat0'] = [i[0] for i in [i.split('_') for i in df_split.cat]]\n",
    "    df_split['cat1'] = [i[1] for i in [i.split('_') for i in df_split.cat]]\n",
    "    df_split['cat2'] = [i[2] for i in [i.split('_') for i in df_split.cat]]\n",
    "    df_split['cat3'] = [i[3] for i in [i.split('_') for i in df_split.cat]]\n",
    "    df_split = df_split.drop('cat', axis=1)\n",
    "        \n",
    "    # Get area data\n",
    "    df_split_area = pd.read_csv(os.path.join(dir05, 'fig_ecozone/df_ecozone_area_data.csv'))\n",
    "    df_split_area['cat0'] = [i[0] for i in [i.split('_') for i in df_split_area.cat]]\n",
    "    df_split_area['cat1'] = [i[1] for i in [i.split('_') for i in df_split_area.cat]]\n",
    "    df_split_area = df_split_area[['cat0', 'cat1', 'area_ha']]\n",
    "        \n",
    "    # Get global data\n",
    "    df_split_all = df_split \\\n",
    "        .assign(carbon_t = df_split.carbon_pg / 1e-9) \\\n",
    "        .groupby(['cat1', 'cat2', 'cat3']) \\\n",
    "        .sum('carbon_t') \\\n",
    "        .reset_index() \\\n",
    "        .pivot(columns='cat3', index=['cat1', 'cat2'], values=['carbon_t']) \\\n",
    "        .droplevel(0, axis=1) \\\n",
    "        .reset_index() \\\n",
    "        .assign(cat0 = 'all')\n",
    "    \n",
    "    df_split_all = df_split_all[['cat0', 'cat1', 'cat2', 'act', 'pot', 'def']]\n",
    "    df_split_all = df_split_all \\\n",
    "        .merge(df_split_area.groupby('cat1').sum('area_ha').reset_index())\n",
    "        \n",
    "    # Get ecozone data\n",
    "    df_split_cat0 = df_split \\\n",
    "        .assign(carbon_t = df_split.carbon_pg / 1e-9) \\\n",
    "        .pivot(columns='cat3', index=['cat0', 'cat1', 'cat2'], values=['carbon_t']) \\\n",
    "        .droplevel(0, axis=1) \\\n",
    "        .reset_index()\n",
    "    \n",
    "    df_split_cat0 = df_split_cat0[['cat0', 'cat1', 'cat2', 'act', 'pot', 'def']]\n",
    "    df_split_cat0 = df_split_cat0.merge(df_split_area, on=['cat0', 'cat1'])\n",
    "        \n",
    "    # Cancatenate global and ecozone data and export scatter plot data\n",
    "    df_concat = pd.concat([df_split_all, df_split_cat0])\n",
    "    df_concat.assign(act_tha = df_concat['act'] / df_concat.area_ha,\n",
    "                     pot_tha = df_concat['pot'] / df_concat.area_ha,\n",
    "                     def_tha = df_concat['def'] / df_concat.area_ha) \\\n",
    "        .reset_index(drop=True) \\\n",
    "        .sort_values('cat2', ascending=False) \\\n",
    "        .to_csv(os.path.join(dir05, 'fig_ecozone/df_ecozone_scatter_data_prim.csv'),\n",
    "                index=False)\n",
    "\n",
    "\n",
    "prep_ecozone_scatter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb76691-e02f-470d-ab9a-c9481139a0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ecozone map data (pgc)\n",
    "def ecozone_map_data_pgc(scen):\n",
    "   \n",
    "    # Get data\n",
    "    df_split = pd.read_csv(\n",
    "        os.path.join(dir05, f'fig_ecozone/df_ecozone_data_{scen}.csv'))\n",
    "    \n",
    "    # Prepare categories\n",
    "    df_split['cat0'] = [i[0] for i in [i.split('_') for i in df_split.cat]]\n",
    "    df_split['cat1'] = [i[1] for i in [i.split('_') for i in df_split.cat]]\n",
    "    df_split['cat2'] = [i[2] for i in [i.split('_') for i in df_split.cat]]\n",
    "    df_split['cat3'] = [i[3] for i in [i.split('_') for i in df_split.cat]]\n",
    "    df_split = df_split.drop('cat', axis=1)\n",
    "\n",
    "    # Pivot, sum, and export\n",
    "    df_split \\\n",
    "        .assign(cat23 = df_split.cat2 + '_' + df_split.cat3) \\\n",
    "        .drop(columns=['cat2', 'cat3']) \\\n",
    "        .pivot(columns=['cat23'], index=['cat0', 'cat1']) \\\n",
    "        .groupby(['cat0']) \\\n",
    "        .sum() \\\n",
    "        .droplevel(0, axis=1) \\\n",
    "        .reset_index() \\\n",
    "        [['cat0', 'cveg_act', 'cveg_pot', 'cveg_def', \n",
    "          'soc_act', 'soc_pot', 'soc_def', \n",
    "          'call_act', 'call_pot', 'call_def']] \\\n",
    "        .to_csv(\n",
    "            os.path.join(dir05, 'fig_ecozone', f'df_ecozone_map_data_{scen}.csv'), \n",
    "            index=False)\n",
    "    \n",
    "    # Print\n",
    "    print(scen)\n",
    "    fstr = f'df_ecozone_map_data_{scen}.csv'\n",
    "    print(round(pd.read_csv(os.path.join(dir05, 'fig_ecozone', fstr))))\n",
    "    \n",
    "\n",
    "#%time ecozone_map_data_pgc('prim')\n",
    "#%time ecozone_map_data_pgc('secd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ce85a5-f446-4bc5-9624-75ee0c0d641b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ecozone map data (pgc)\n",
    "def ecozone_map_data_pgc_new(scen):\n",
    "\n",
    "    # Select dataset\n",
    "    if scen == 'prim':\n",
    "        ds = ds_prim\n",
    "    if scen == 'secd':\n",
    "        ds = ds_secd\n",
    "\n",
    "    # Dataset with carbon stocks for each combination of ecozones\n",
    "    ds_split = xr.Dataset()\n",
    "    for i in list(ds_fao_split.data_vars):\n",
    "            for iii in list(ds.data_vars):\n",
    "                ds_split[i[8:] + '_' + iii] = (\n",
    "                    ds_fao_split[i] * ds[iii])\n",
    "    \n",
    "    # Dataframe with carbons stocks of each combination\n",
    "    df_split = (ds_split * da_area * 1e-9) \\\n",
    "        .sum(['lat', 'lon']) \\\n",
    "        .compute() \\\n",
    "        .to_pandas() \\\n",
    "        .reset_index() \\\n",
    "        .rename(columns={'index':'cat', 0: 'carbon_pg'})\n",
    "    \n",
    "    # Prepare categories\n",
    "    df_split['cat0'] = [i[0] for i in [i.split('_') for i in df_split.cat]]\n",
    "    df_split['cat1'] = [i[1] for i in [i.split('_') for i in df_split.cat]]\n",
    "    df_split['cat2'] = [i[2] for i in [i.split('_') for i in df_split.cat]]\n",
    "    \n",
    "    # Pivot dataframe and prepare for export\n",
    "    df_split \\\n",
    "        .drop('cat', axis=1) \\\n",
    "        .assign(cat12 = df_split.cat1 + '_' + df_split.cat2) \\\n",
    "        .drop(columns=['cat1', 'cat2']) \\\n",
    "        .pivot(columns=['cat12'], index=['cat0']) \\\n",
    "        .groupby(['cat0']) \\\n",
    "        .sum() \\\n",
    "        .droplevel(0, axis=1) \\\n",
    "        .reset_index() \\\n",
    "        [['cat0', 'cveg_act', 'cveg_pot', 'cveg_def', \n",
    "          'soc_act', 'soc_pot', 'soc_def', \n",
    "          'call_act', 'call_pot', 'call_def']] \\\n",
    "        .to_csv(\n",
    "            os.path.join(dir05, 'fig_ecozone', f'df_ecozone_map_data_{scen}.csv'), \n",
    "            index=False)\n",
    "    \n",
    "    # Print\n",
    "    print(scen)\n",
    "    fstr = f'df_ecozone_map_data_{scen}.csv'\n",
    "    print(round(pd.read_csv(os.path.join(dir05, 'fig_ecozone', fstr))))\n",
    "    \n",
    "\n",
    "%time ecozone_map_data_pgc_new('prim')\n",
    "%time ecozone_map_data_pgc_new('secd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7561aafe-d4d2-4e77-890d-68a45ec11153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ecozone bar data (pgc)\n",
    "def ecozone_map_data_pgc(scen):\n",
    "\n",
    "    # Get data\n",
    "    df_split = pd.read_csv(\n",
    "        os.path.join(dir05 + f'fig_ecozone/df_ecozone_data_{scen}.csv'))\n",
    "    \n",
    "    # Prepare categories\n",
    "    df_split = df_split.assign(\n",
    "        cat0 = [i[0] for i in [i.split('_') for i in df_split.cat]],\n",
    "        cat1 = [i[1] for i in [i.split('_') for i in df_split.cat]],\n",
    "        cat2 = [i[2] for i in [i.split('_') for i in df_split.cat]],\n",
    "        cat3 = [i[3] for i in [i.split('_') for i in df_split.cat]]) \\\n",
    "        .drop('cat', axis=1)\n",
    "    \n",
    "    # Pivot\n",
    "    df_split_bar = df_split[df_split.cat3 == 'def'] \\\n",
    "        .pivot(columns='cat1', index=['cat0', 'cat2'], values='carbon_pg')\n",
    "    \n",
    "    # Create ambiguos class, export\n",
    "    df_split_bar = df_split_bar \\\n",
    "        .assign(ambig = df_split_bar.forestn + df_split_bar.other + df_split_bar.shrub) \\\n",
    "        .reset_index() \\\n",
    "        .sort_values(['cat2', 'cat0'], ascending=[False, True]) \\\n",
    "        .to_csv(\n",
    "            os.path.join(dir05 + f'fig_ecozone/df_ecozone_bar_data_{scen}.csv'),\n",
    "        index=False)\n",
    "    \n",
    "    # Print\n",
    "    print(scen)\n",
    "    fstr = f'df_ecozone_bar_data_{scen}.csv'\n",
    "    print(round(pd.read_csv(os.path.join(dir05 + f'fig_ecozone/{fstr}')), 0))\n",
    "    \n",
    "ecozone_map_data_pgc('prim')\n",
    "ecozone_map_data_pgc('secd')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a21379-d483-4126-b9d3-6c1d36fc71ce",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6458708d-1a0f-4f4a-a74e-e6d186b65488",
   "metadata": {},
   "source": [
    "### Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5fe61a-8251-4923-a71d-498c73d5b272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average carbon per area for each carbon pool and each luc class (tha and %)\n",
    "# Get data\n",
    "df = pd.read_csv(\n",
    "    os.path.join(dir05, 'fig_ecozone/df_ecozone_scatter_data_prim.csv'))\n",
    "df = df[df.cat0 == 'all'] \\\n",
    "    [['cat1', 'cat2', 'act_tha', 'pot_tha', 'def_tha']] \\\n",
    "    .assign(def_p = round(1-(df.act_tha / df.pot_tha), 2)) \\\n",
    "    .sort_values('cat1', ascending=True) \\\n",
    "    .sort_values('cat2', ascending=False) \\\n",
    "\n",
    "\n",
    "for i in ['act_tha', 'pot_tha', 'def_tha']:\n",
    "    df[i] = round(df[i]).astype('int')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607b709e-41fd-46f4-8773-c608504e73f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average carbon per area for each carbon pool and each ecozone (tha and %)\n",
    "df = pd.read_csv(\n",
    "    os.path.join(dir05, 'fig_ecozone/df_ecozone_scatter_data_prim.csv'))\n",
    "round(df[df.cat1 == 'foresth'][['cat0', 'cat2', 'act_tha', 'pot_tha', 'def_tha']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5c3586-28c5-482c-881d-4480c1fbea70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global deficit stats luc classes (PgC)\n",
    "df = pd.read_csv(\n",
    "    os.path.join(dir05, 'fig_ecozone/df_ecozone_scatter_data_prim.csv'))\n",
    "\n",
    "df[df.cat0 == 'all'][['cat1', 'cat2', 'def']] \\\n",
    "    .assign(d = round(df['def']  * 1e-9).astype('int'))[['cat1', 'cat2', 'd']] \\\n",
    "    .sort_values('cat1', ascending=True) \\\n",
    "    .sort_values('cat2', ascending=False) \\\n",
    "    .reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df12123-d6f2-4f81-a136-d8559c8f3e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global deficit stats ecozone (PgC)\n",
    "df = pd.read_csv(\n",
    "    os.path.join(dir05, 'fig_ecozone/df_ecozone_scatter_data_prim.csv'))\n",
    "df['def'] = df['def']  * 1e-9\n",
    "round(df[df.cat1 == 'foresth'][['cat0', 'cat1', 'cat2', 'def']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c32782c-c816-4cfc-8ab8-c1736123693d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a84bddd-e889-419e-831c-5caf2cf6361d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (deficit)",
   "language": "python",
   "name": "deficit_python312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2c2b899-ef03-4719-a476-0d331bb9bd49",
   "metadata": {},
   "source": [
    "# Prepare ESA CCI luc data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97882d61-bc15-450a-b678-1e58ae57ab15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import os, time, shutil, rioxarray\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from scipy import ndimage\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b974824-2159-4916-8e1c-ffe68fbb83e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories\n",
    "dir_data =  '../data/'\n",
    "dir01 = '../paper_deficit/output/01_prep/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c24807-6128-48cf-a440-28668a4a53fd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8f0114-b184-4617-bfbe-0c7a7f1590df",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e11c1f-2f8f-42bd-b261-5b172a015885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "from dask_jobqueue import SLURMCluster\n",
    "from dask.distributed import Client\n",
    "import dask\n",
    "\n",
    "# Initialize dask\n",
    "cluster = SLURMCluster(\n",
    "    queue='compute',                      # SLURM queue to use\n",
    "    cores=32,                             # Number of CPU cores per job\n",
    "    memory='256 GB',                      # Memory per job\n",
    "    account='bm0891',                     # Account allocation\n",
    "    interface=\"ib0\",                      # Network interface for communication\n",
    "    walltime='06:00:00',                  # Maximum runtime per job\n",
    "    local_directory='../dask/',           # Directory for local storage\n",
    "    job_extra_directives=[                # Additional SLURM directives for logging\n",
    "        '-o ../dask/LOG_worker_%j.o',     # Output log\n",
    "        '-e ../dask/LOG_worker_%j.e'      # Error log\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Scale dask cluster\n",
    "cluster.scale(jobs=10)\n",
    "\n",
    "# Configurate dashboard url\n",
    "dask.config.config.get('distributed').get('dashboard').update(\n",
    "    {'link': '{JUPYTERHUB_SERVICE_PREFIX}/proxy/{port}/status'}\n",
    ")\n",
    "\n",
    "# Create client\n",
    "client = Client(cluster)\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e299ea-d950-4f5b-ba05-4c9d1c5b5d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to trim memory of workers\n",
    "def trim_memory() -> int:\n",
    "    import ctypes\n",
    "    libc = ctypes.CDLL(\"libc.so.6\")\n",
    "    return libc.malloc_trim(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ed85d1-7d35-4533-a4f7-2bdd40fafc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_esacci():\n",
    "    \n",
    "    \"\"\"Prepare stable ESA-CCI grid cell data by processing land cover classes.\"\"\"\n",
    "    \n",
    "    # Step 1: Load ESA-CCI land cover datasets\n",
    "    esacci = xr.open_mfdataset(dir_data + 'esa_luc/*.nc', combine='by_coords') \\\n",
    "        .chunk(dict(lat=5000, lon=5000, time=1)) \\\n",
    "        .lccs_class\n",
    "\n",
    "    # Step 2: Convert the time dimension to just the year\n",
    "    esacci['time'] = pd.DatetimeIndex(esacci.time.data).year\n",
    "\n",
    "    # Step 3: Process each land cover type (based on 'flag_values' attribute)\n",
    "    for i in esacci.attrs['flag_values']:\n",
    "        # Create a new dataset marking cells where the specific land cover type (i) is present\n",
    "        esacci2 = xr.Dataset()\n",
    "        esacci2['esacci_' + str(i)] = xr.where(esacci == i, True, False)\n",
    "        \n",
    "        # Step 4: Save the new dataset for the specific land cover type as a Zarr file\n",
    "        esacci2.to_zarr(\n",
    "            dir01 + 'esacci_prep/esacci_prep_' + str(i) + '.zarr', mode='w')\n",
    "\n",
    "        # Step 5: Reload the saved dataset and re-chunk it\n",
    "        esacci2 = xr.open_zarr(\n",
    "            dir01 + 'esacci_prep/esacci_prep_' + str(i) + '.zarr') \\\n",
    "            .chunk(dict(lat=5000, lon=5000, time=-1))\n",
    "\n",
    "        # Step 6: Identify cells without land-use change (LUC) over the years (no change = stable land cover)\n",
    "        # If the sum over time equals 29 (indicating presence for all 29 years), it's a stable land cover\n",
    "        xr.where(esacci2.sum('time') == 29, True, False) \\\n",
    "            .to_zarr(dir01 + 'esacci_prep/esacci_prep_sum_' + str(i) + '.zarr', \n",
    "                     mode='w')\n",
    "        \n",
    "        # Release memory after processing each land cover type\n",
    "        client.run(trim_memory)\n",
    "\n",
    "    # Step 7: Open all the prepared land cover stability datasets\n",
    "    esacci_prep_sum = xr.open_mfdataset(\n",
    "        dir01 + 'esacci_prep/esacci_prep_sum_*.zarr', engine='zarr')\n",
    "\n",
    "    # Step 8: Identify stable grid cells for specific land cover types\n",
    "    # Stable crop grid cells (classes 10, 11, 12, 20)\n",
    "    esacci_crop = (\n",
    "        esacci_prep_sum.esacci_10 + esacci_prep_sum.esacci_11 +\n",
    "        esacci_prep_sum.esacci_12 + esacci_prep_sum.esacci_20\n",
    "    )\n",
    "    \n",
    "    # Stable pasture grid cells (class 130)\n",
    "    esacci_pasture = esacci_prep_sum.esacci_130\n",
    "    \n",
    "    # Stable forest and shrubland grid cells (many classes combined)\n",
    "    esacci_fso = (\n",
    "        esacci_prep_sum.esacci_50 + esacci_prep_sum.esacci_60 +\n",
    "        esacci_prep_sum.esacci_61 + esacci_prep_sum.esacci_62 +\n",
    "        esacci_prep_sum.esacci_70 + esacci_prep_sum.esacci_71 +\n",
    "        esacci_prep_sum.esacci_72 + esacci_prep_sum.esacci_80 +\n",
    "        esacci_prep_sum.esacci_81 + esacci_prep_sum.esacci_82 +\n",
    "        esacci_prep_sum.esacci_90 + esacci_prep_sum.esacci_100 +\n",
    "        esacci_prep_sum.esacci_110 + esacci_prep_sum.esacci_120 +\n",
    "        esacci_prep_sum.esacci_121 + esacci_prep_sum.esacci_122 +\n",
    "        esacci_prep_sum.esacci_140 + esacci_prep_sum.esacci_150 +\n",
    "        esacci_prep_sum.esacci_151 + esacci_prep_sum.esacci_152 +\n",
    "        esacci_prep_sum.esacci_153 + esacci_prep_sum.esacci_160 +\n",
    "        esacci_prep_sum.esacci_170 + esacci_prep_sum.esacci_180 +\n",
    "        esacci_prep_sum.esacci_190 + esacci_prep_sum.esacci_200 +\n",
    "        esacci_prep_sum.esacci_201 + esacci_prep_sum.esacci_202\n",
    "    )\n",
    "\n",
    "    # Step 9: Create a dataset containing stable grid cells for crops, pasture, and forest/shrubland (FSO)\n",
    "    esacci_prep_stable = xr.Dataset({\n",
    "        'esacci_stable_crop': esacci_crop,\n",
    "        'esacci_stable_pasture': esacci_pasture,\n",
    "        'esacci_stable_fso': esacci_fso\n",
    "    })\n",
    "\n",
    "    # Step 10: Export the stable grid cell dataset as a Zarr file\n",
    "    esacci_prep_stable.to_zarr(dir01 + 'esacci_prep_stable.zarr', mode='w')\n",
    "\n",
    "    # Step 11: Reload the stable grid cell dataset\n",
    "    esacci_prep_stable = xr.open_zarr(dir01 + 'esacci_prep_stable.zarr')\n",
    "\n",
    "    # Step 12: Export each stable grid cell variable as GeoTIFF\n",
    "    for var in esacci_prep_stable.data_vars:\n",
    "        esacci_prep_stable[var] \\\n",
    "            .rename(dict(lat='y', lon='x')) \\\n",
    "            .astype('float32') \\\n",
    "            .rio.to_raster(dir01 + var + '.tif')\n",
    "\n",
    "        # Release memory after each export\n",
    "        client.run(trim_memory)\n",
    "\n",
    "    # Step 13: Clean up intermediate files and directories\n",
    "    shutil.rmtree(dir01 + 'esacci_prep')  # Remove temporary directories\n",
    "    shutil.rmtree(dir01 + 'esacci_prep_stable.zarr')  # Remove Zarr directory after TIFF export\n",
    "\n",
    "\n",
    "%time prep_esacci()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7147a1-3671-4ae5-aad2-f0de5ae2eae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close dask cluster\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227c47f5-b220-4ce3-a48a-dc05d21e6568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for 60s for dask client to completely disconnect\n",
    "time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b71dfd4-1dc0-476f-8ea8-6a2041f98f97",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44a013e-8480-4401-8473-a02f1c4e83fd",
   "metadata": {},
   "source": [
    "### Regridding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0beef9-e67b-4960-8cbf-bf2393444077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import regridding function\n",
    "from regrid_high_res_v1_01 import regrid_high_res, prep_tif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c28cf6-5869-4f88-b9ef-ab65291ac50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regrid_da(f_source, dir_target, dir_source, dir_out, \n",
    "              size_tiles, fill_value=None, olap=1):  \n",
    "    \"\"\"Regrid large xarray dataarrays.\n",
    "\n",
    "    Args:\n",
    "        f_source (str): The filename (without extension) of the source .tif file to be regridded.\n",
    "        dir_target (str): Directory containing target grid .tif file.\n",
    "        dir_source (str): Directory containing the the source  .tif file.\n",
    "        dir_out (str): Directory to store the output and intermediate files.\n",
    "        size_tiles (int): Size of the regridding tiles in degrees.\n",
    "        fill_value (float, optional): Fill value to use in the regridding process. Defaults to None.\n",
    "        olap (int, optional): Overlap size in degrees for regridding tiles. Defaults to 1.\n",
    "        \n",
    "    Returns:\n",
    "        xarray.Dataset: The combined dataset after regridding.\n",
    "    \"\"\"\n",
    "    # Prepare the target and source data arrays from TIFF files\n",
    "    da_target = prep_tif(dir_target + 'target_grid.tif', 'target_grid')\n",
    "    da_source = prep_tif(dir_source + f_source + '.tif', f_source)\n",
    "    # Regridd source array to target grid\n",
    "    regrid_high_res(da_target, da_source, dir_out,\n",
    "                    account='bm0891', partition='compute',\n",
    "                    size_tiles=size_tiles, olap=olap, fill_value = fill_value,\n",
    "                    type_export='zarr', del_interm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5888c05c-aa81-43ef-bd2a-f717978fb867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regridding\n",
    "list_esacci = ['esacci_stable_crop', 'esacci_stable_pasture', \n",
    "               'esacci_stable_fso']\n",
    "\n",
    "for i in list_esacci:\n",
    "    print(i)\n",
    "    %time regrid_da(i, dir01, dir01, dir01, 30, 127, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c833a0-8fde-4489-81ab-7b7ddcb2e387",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9a5a6d-af0d-440b-91a4-6e6cb6929895",
   "metadata": {},
   "source": [
    "### Fill nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007d9ace-8fc3-471d-9932-c17db22aeb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_nans(var, dir_out):\n",
    "    \"\"\"\n",
    "    Fills NaN values in the specified variable's dataset using the nearest valid \n",
    "    data, applies a land mask, and exports the result to a new Zarr dataset.\n",
    "\n",
    "    Args:\n",
    "        var (str): Name of the variable to process (e.g., 'temperature', 'precipitation').\n",
    "        dir_out (str): Directory where prepared data is stored and the filled dataset will be exported.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    def fill_nans_array(data, invalid):\n",
    "        \"\"\"\n",
    "        Replace invalid (NaN) data cells by the value of the nearest valid data \n",
    "        cell.\n",
    "        \"\"\"\n",
    "        ind = ndimage.distance_transform_edt(invalid,\n",
    "                                             return_distances=False,\n",
    "                                             return_indices=True)\n",
    "        return data[tuple(ind)]\n",
    "\n",
    "    # Paths for input and output\n",
    "    land_mask_path = os.path.join(dir_out, 'ds_prep_copernicus_land_mask.zarr')\n",
    "    var_data_path = os.path.join(dir_out, f'ds_regridded_{var}.zarr')\n",
    "    output_path = os.path.join(dir_out, f'ds_prep_{var}.zarr')\n",
    "\n",
    "    # Read land mask data\n",
    "    da_land = xr.open_zarr(land_mask_path) \\\n",
    "                .chunk(dict(lat=5000, lon=5000)) \\\n",
    "                .copernicus_land_mask \\\n",
    "                .compute()\n",
    "\n",
    "    # Read variable data\n",
    "    da_var = xr.open_zarr(var_data_path)['regridded_' + var]\n",
    "\n",
    "    # Fill nan using function fill_nans_array\n",
    "    # If there are no NaNs, skip filling process\n",
    "    if not da_var.isnull().any():\n",
    "        da_fill = da_var.values  # No filling required\n",
    "    else:\n",
    "        da_fill = fill_nans_array(da_var.values, da_var.isnull().values)\n",
    "\n",
    "    # Create a new Dataset with filled data\n",
    "    ds_filled = xr.Dataset(dict(lat = da_var.lat, lon=da_var.lon))\n",
    "    ds_filled[var] = (('lat', 'lon'), da_fill)\n",
    "\n",
    "    # Apply land mask to the filled data\n",
    "    ds_filled = ds_filled.where(da_land)\n",
    "\n",
    "    # Export the filled dataset to Zarr format\n",
    "    ds_filled.chunk(dict(lat=5000, lon=5000)) \\\n",
    "             .to_zarr(output_path, mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c1b5e8-bc9c-4652-baab-fea9a84515d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in ['esacci_stable_crop', 'esacci_stable_pasture', \n",
    "          'esacci_stable_fso']:\n",
    "    %time fill_nans(i, dir01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938ad8b0-5cab-4c1f-86f9-aea2adc68c23",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55eea6c-46f2-4627-8107-4301d06865af",
   "metadata": {},
   "source": [
    "### Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bf086a-a991-406c-8a14-ebd925720b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot to check\n",
    "for i in ['esacci_stable_crop', 'esacci_stable_pasture', \n",
    "          'esacci_stable_fso']:\n",
    "    fig, ax = plt.subplots(figsize=(10, 5), ncols=1, nrows=1)\n",
    "    xr.open_zarr(dir01 + 'ds_prep_' + i + '.zarr')[i] \\\n",
    "        .plot.imshow(ax=ax)\n",
    "    ax.set_title(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (deficit)",
   "language": "python",
   "name": "deficit_python312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
